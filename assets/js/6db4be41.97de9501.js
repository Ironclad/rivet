"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1673],{5591:(e,t,a)=>{a.d(t,{xA:()=>p,yg:()=>y});var n=a(8527);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function l(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?l(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},l=Object.keys(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(n=0;n<l.length;n++)a=l[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var u=n.createContext({}),s=function(e){var t=n.useContext(u),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=s(e.components);return n.createElement(u.Provider,{value:t},e.children)},g="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,l=e.originalType,u=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),g=s(a),c=r,y=g["".concat(u,".").concat(c)]||g[c]||d[c]||l;return a?n.createElement(y,i(i({ref:t},p),{},{components:a})):n.createElement(y,i({ref:t},p))}));function y(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=a.length,i=new Array(l);i[0]=c;var o={};for(var u in t)hasOwnProperty.call(t,u)&&(o[u]=t[u]);o.originalType=e,o[g]="string"==typeof e?e:r,i[1]=o;for(var s=2;s<l;s++)i[s]=a[s];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},7801:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>u,contentTitle:()=>i,default:()=>d,frontMatter:()=>l,metadata:()=>o,toc:()=>s});var n=a(4465),r=(a(8527),a(5591));const l={sidebar_label:"Autoevals"},i="Autoevals Plugin",o={unversionedId:"user-guide/plugins/built-in/autoevals",id:"user-guide/plugins/built-in/autoevals",title:"Autoevals Plugin",description:"The Autoevals plugin provides a node that can automatically evaluate the performance of an LLM response using a battle-tested set of prompts.",source:"@site/docs/user-guide/plugins/built-in/autoevals.md",sourceDirName:"user-guide/plugins/built-in",slug:"/user-guide/plugins/built-in/autoevals",permalink:"/docs/user-guide/plugins/built-in/autoevals",draft:!1,editUrl:"https://github.com/ironclad/rivet/tree/main/packages/docs/docs/user-guide/plugins/built-in/autoevals.md",tags:[],version:"current",frontMatter:{sidebar_label:"Autoevals"},sidebar:"userGuide",previous:{title:"AssemblyAI",permalink:"/docs/user-guide/plugins/built-in/assemblyai"},next:{title:"Gentrace",permalink:"/docs/user-guide/plugins/built-in/gentrace"}},u={},s=[{value:"Nodes",id:"nodes",level:2},{value:"Autoevals Node",id:"autoevals-node",level:3},{value:"Inputs",id:"inputs",level:4},{value:"Outputs",id:"outputs",level:4},{value:"Editor Settings",id:"editor-settings",level:4},{value:"Evaluations",id:"evaluations",level:2}],p={toc:s},g="wrapper";function d(e){let{components:t,...l}=e;return(0,r.yg)(g,(0,n.A)({},p,l,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"autoevals-plugin"},"Autoevals Plugin"),(0,r.yg)("p",null,"The Autoevals plugin provides a node that can automatically evaluate the performance of an LLM response using a battle-tested set of prompts."),(0,r.yg)("p",null,"For more information on autoevals, see ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/braintrustdata/autoevals"},"its documentation"),"."),(0,r.yg)("h2",{id:"nodes"},"Nodes"),(0,r.yg)("h3",{id:"autoevals-node"},"Autoevals Node"),(0,r.yg)("p",null,(0,r.yg)("img",{alt:"Autoevals Node",src:a(705).A,width:"870",height:"452"})),(0,r.yg)("h4",{id:"inputs"},"Inputs"),(0,r.yg)("p",null,"The inputs to the Autoevals node depend on the configured evaluation being performed. All types will have an\n",(0,r.yg)("inlineCode",{parentName:"p"},"output")," port - This is the LLM response that will be evaluated. For other ports, see the table below."),(0,r.yg)("h4",{id:"outputs"},"Outputs"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Title"),(0,r.yg)("th",{parentName:"tr",align:null},"Data Type"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"),(0,r.yg)("th",{parentName:"tr",align:null},"Notes"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Score"),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"number")),(0,r.yg)("td",{parentName:"tr",align:null},"The score that has been given to the response, from 0 to 1. A 0 indicates complete failure, and a 1 indicates a complete pass."),(0,r.yg)("td",{parentName:"tr",align:null})),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Rationale"),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"string")),(0,r.yg)("td",{parentName:"tr",align:null},"The rationale for the score that has been given to the response."),(0,r.yg)("td",{parentName:"tr",align:null})),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Metadata"),(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("inlineCode",{parentName:"td"},"object")),(0,r.yg)("td",{parentName:"tr",align:null},"The complete metadata associated with the autoevals evaluation, including the rationale and any other information specific to the type selected."),(0,r.yg)("td",{parentName:"tr",align:null})))),(0,r.yg)("h4",{id:"editor-settings"},"Editor Settings"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Setting"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"),(0,r.yg)("th",{parentName:"tr",align:null},"Default Value"),(0,r.yg)("th",{parentName:"tr",align:null},"Use Input Toggle"),(0,r.yg)("th",{parentName:"tr",align:null},"Input Data Type"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"Evaluator"),(0,r.yg)("td",{parentName:"tr",align:null},"The evaluation that will be performed on the input."),(0,r.yg)("td",{parentName:"tr",align:null},"Factuality"),(0,r.yg)("td",{parentName:"tr",align:null},"No"),(0,r.yg)("td",{parentName:"tr",align:null},"N/A")))),(0,r.yg)("h2",{id:"evaluations"},"Evaluations"),(0,r.yg)("p",null,"See the ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/braintrustdata/autoevals"},"autoevals documentation")))}d.isMDXComponent=!0},705:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/autoevals-node-672adfa9b998df7e64078b8e38d79c19.png"}}]);