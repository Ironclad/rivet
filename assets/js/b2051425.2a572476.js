"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[336],{1042:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>h});var a=n(3249);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},l=Object.keys(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var p=a.createContext({}),s=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=s(e.components);return a.createElement(p.Provider,{value:t},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,l=e.originalType,p=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),d=s(n),c=r,h=d["".concat(p,".").concat(c)]||d[c]||m[c]||l;return n?a.createElement(h,o(o({ref:t},u),{},{components:n})):a.createElement(h,o({ref:t},u))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=n.length,o=new Array(l);o[0]=c;var i={};for(var p in t)hasOwnProperty.call(t,p)&&(i[p]=t[p]);i.originalType=e,i[d]="string"==typeof e?e:r,o[1]=i;for(var s=2;s<l;s++)o[s]=n[s];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},5089:(e,t,n)=>{n.d(t,{Z:()=>o});var a=n(3249),r=n(2689);const l={tabItem:"tabItem_o9Hs"};function o(e){let{children:t,hidden:n,className:o}=e;return a.createElement("div",{role:"tabpanel",className:(0,r.Z)(l.tabItem,o),hidden:n},t)}},2327:(e,t,n)=>{n.d(t,{Z:()=>v});var a=n(7396),r=n(3249),l=n(2689),o=n(5986),i=n(6659),p=n(4532),s=n(5821),u=n(3312);function d(e){return function(e){return r.Children.map(e,(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:t}=e;return!!t&&"object"==typeof t&&"value"in t}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}(e).map((e=>{let{props:{value:t,label:n,attributes:a,default:r}}=e;return{value:t,label:n,attributes:a,default:r}}))}function m(e){const{values:t,children:n}=e;return(0,r.useMemo)((()=>{const e=t??d(n);return function(e){const t=(0,s.l)(e,((e,t)=>e.value===t.value));if(t.length>0)throw new Error(`Docusaurus error: Duplicate values "${t.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[t,n])}function c(e){let{value:t,tabValues:n}=e;return n.some((e=>e.value===t))}function h(e){let{queryString:t=!1,groupId:n}=e;const a=(0,i.k6)(),l=function(e){let{queryString:t=!1,groupId:n}=e;if("string"==typeof t)return t;if(!1===t)return null;if(!0===t&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:t,groupId:n});return[(0,p._X)(l),(0,r.useCallback)((e=>{if(!l)return;const t=new URLSearchParams(a.location.search);t.set(l,e),a.replace({...a.location,search:t.toString()})}),[l,a])]}function g(e){const{defaultValue:t,queryString:n=!1,groupId:a}=e,l=m(e),[o,i]=(0,r.useState)((()=>function(e){let{defaultValue:t,tabValues:n}=e;if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(t){if(!c({value:t,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${t}" but none of its children has the corresponding value. Available values are: ${n.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return t}const a=n.find((e=>e.default))??n[0];if(!a)throw new Error("Unexpected error: 0 tabValues");return a.value}({defaultValue:t,tabValues:l}))),[p,s]=h({queryString:n,groupId:a}),[d,g]=function(e){let{groupId:t}=e;const n=function(e){return e?`docusaurus.tab.${e}`:null}(t),[a,l]=(0,u.Nk)(n);return[a,(0,r.useCallback)((e=>{n&&l.set(e)}),[n,l])]}({groupId:a}),k=(()=>{const e=p??d;return c({value:e,tabValues:l})?e:null})();(0,r.useLayoutEffect)((()=>{k&&i(k)}),[k]);return{selectedValue:o,selectValue:(0,r.useCallback)((e=>{if(!c({value:e,tabValues:l}))throw new Error(`Can't select invalid tab value=${e}`);i(e),s(e),g(e)}),[s,g,l]),tabValues:l}}var k=n(3072);const N={tabList:"tabList_Dwv5",tabItem:"tabItem_KiUr"};function f(e){let{className:t,block:n,selectedValue:i,selectValue:p,tabValues:s}=e;const u=[],{blockElementScrollPositionUntilNextRender:d}=(0,o.o5)(),m=e=>{const t=e.currentTarget,n=u.indexOf(t),a=s[n].value;a!==i&&(d(t),p(a))},c=e=>{let t=null;switch(e.key){case"Enter":m(e);break;case"ArrowRight":{const n=u.indexOf(e.currentTarget)+1;t=u[n]??u[0];break}case"ArrowLeft":{const n=u.indexOf(e.currentTarget)-1;t=u[n]??u[u.length-1];break}}t?.focus()};return r.createElement("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,l.Z)("tabs",{"tabs--block":n},t)},s.map((e=>{let{value:t,label:n,attributes:o}=e;return r.createElement("li",(0,a.Z)({role:"tab",tabIndex:i===t?0:-1,"aria-selected":i===t,key:t,ref:e=>u.push(e),onKeyDown:c,onClick:m},o,{className:(0,l.Z)("tabs__item",N.tabItem,o?.className,{"tabs__item--active":i===t})}),n??t)})))}function b(e){let{lazy:t,children:n,selectedValue:a}=e;const l=(Array.isArray(n)?n:[n]).filter(Boolean);if(t){const e=l.find((e=>e.props.value===a));return e?(0,r.cloneElement)(e,{className:"margin-top--md"}):null}return r.createElement("div",{className:"margin-top--md"},l.map(((e,t)=>(0,r.cloneElement)(e,{key:t,hidden:e.props.value!==a}))))}function y(e){const t=g(e);return r.createElement("div",{className:(0,l.Z)("tabs-container",N.tabList)},r.createElement(f,(0,a.Z)({},e,t)),r.createElement(b,(0,a.Z)({},e,t)))}function v(e){const t=(0,k.Z)();return r.createElement(y,(0,a.Z)({key:String(t)},e))}},2286:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>p,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var a=n(7396),r=(n(3249),n(1042)),l=n(2327),o=n(5089);const i={id:"chat",title:"Chat Node",sidebar_label:"Chat"},p=void 0,s={unversionedId:"node-reference/chat",id:"node-reference/chat",title:"Chat Node",description:"Overview",source:"@site/docs/node-reference/chat.mdx",sourceDirName:"node-reference",slug:"/node-reference/chat",permalink:"/docs/node-reference/chat",draft:!1,editUrl:"https://github.com/ironclad/rivet/tree/main/packages/docs/docs/node-reference/chat.mdx",tags:[],version:"current",frontMatter:{id:"chat",title:"Chat Node",sidebar_label:"Chat"},sidebar:"nodeReference",previous:{title:"Assemble Prompt",permalink:"/docs/node-reference/assemble-prompt"},next:{title:"GPT Function",permalink:"/docs/node-reference/gpt-function"}},u={},d=[{value:"Overview",id:"overview",level:2},{value:"Inputs",id:"inputs",level:2},{value:"Outputs",id:"outputs",level:2},{value:"Editor Settings",id:"editor-settings",level:2},{value:"Example 1: Simple Response",id:"example-1-simple-response",level:3},{value:"Example 2: Connecting to LM Studio",id:"example-2-connecting-to-lm-studio",level:3},{value:"Error Handling",id:"error-handling",level:2},{value:"FAQ",id:"faq",level:2},{value:"See Also",id:"see-also",level:2}],m={toc:d},c="wrapper";function h(e){let{components:t,...i}=e;return(0,r.kt)(c,(0,a.Z)({},m,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"overview"},"Overview"),(0,r.kt)("p",null,"The Chat Node send one or more messages to an LLM - OpenAI's GPT, or any API compatible with the OpenAI API. It then returns the response from the LLM."),(0,r.kt)("p",null,"You can use the Chat Node for local LLMs, as long as their API is compatible with the OpenAI API. For example, you can use the Chat Node with ",(0,r.kt)("a",{parentName:"p",href:"https://lmstudio.ai/"},"LM Studio"),"."),(0,r.kt)("p",null,"If you are looking for other language models that do not support the OpenAI API format, see the Plugins page for a list of available plugins that implement other language model nodes."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Chat Node Screenshot",src:n(8729).Z,width:"472",height:"436"})),(0,r.kt)(l.Z,{defaultValue:"inputs",values:[{label:"Inputs",value:"inputs"},{label:"Outputs",value:"outputs"},{label:"Editor Settings",value:"settings"}],mdxType:"Tabs"},(0,r.kt)(o.Z,{value:"inputs",mdxType:"TabItem"},(0,r.kt)("h2",{id:"inputs"},"Inputs"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Title"),(0,r.kt)("th",{parentName:"tr",align:null},"Data Type"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"Default Value"),(0,r.kt)("th",{parentName:"tr",align:null},"Notes"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"System Prompt"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"string")," or ",(0,r.kt)("inlineCode",{parentName:"td"},"chat-message")),(0,r.kt)("td",{parentName:"tr",align:null},"A convenience input that allows a system prompt to be prepended to the main prompt message/messages."),(0,r.kt)("td",{parentName:"tr",align:null},"(None)"),(0,r.kt)("td",{parentName:"tr",align:null},"If not connected, then no system prompt will be prepended. You can always include a system prompt in the main prompt input instead, if you like instead, using an ",(0,r.kt)("a",{parentName:"td",href:"/docs/node-reference/assemble-prompt"},"Assemble Prompt")," node.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Prompt"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"string")," / ",(0,r.kt)("inlineCode",{parentName:"td"},"string[]")," / ",(0,r.kt)("inlineCode",{parentName:"td"},"chat-message")," / ",(0,r.kt)("inlineCode",{parentName:"td"},"chat-message[]")),(0,r.kt)("td",{parentName:"tr",align:null},"The main prompt to send to the language model. Can be one or more strings or chat-messages."),(0,r.kt)("td",{parentName:"tr",align:null},"(Empty list)"),(0,r.kt)("td",{parentName:"tr",align:null},"Strings will be converted into chat messages of type ",(0,r.kt)("inlineCode",{parentName:"td"},"user"),", with no name.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Functions"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"gpt-function")," or ",(0,r.kt)("inlineCode",{parentName:"td"},"gpt-function[]")),(0,r.kt)("td",{parentName:"tr",align:null},"Defines the available functions that GPT is allowed to call during its response."),(0,r.kt)("td",{parentName:"tr",align:null},"(Required)"),(0,r.kt)("td",{parentName:"tr",align:null},"Only enabled if the ",(0,r.kt)("inlineCode",{parentName:"td"},"Enable Function Use")," setting is enabled."))))),(0,r.kt)(o.Z,{value:"outputs",mdxType:"TabItem"},(0,r.kt)("h2",{id:"outputs"},"Outputs"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Title"),(0,r.kt)("th",{parentName:"tr",align:null},"Data Type"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"Notes"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Response"),(0,r.kt)("td",{parentName:"tr",align:null},"string"),(0,r.kt)("td",{parentName:"tr",align:null},"The response from GPT."),(0,r.kt)("td",{parentName:"tr",align:null},"The response will be streamed in, but subsequent nodes will not be executed until the response is finished."))))),(0,r.kt)(o.Z,{value:"settings",mdxType:"TabItem"},(0,r.kt)("h2",{id:"editor-settings"},"Editor Settings"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Setting"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"Default Value"),(0,r.kt)("th",{parentName:"tr",align:null},"Use Input Toggle"),(0,r.kt)("th",{parentName:"tr",align:null},"Input Data Type"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Model"),(0,r.kt)("td",{parentName:"tr",align:null},"The GPT model to use for the request. If you are not using OpenAI's GPT, and you would like to set the ",(0,r.kt)("inlineCode",{parentName:"td"},"model")," parameter, you can see the Custom Model setting below."),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"gpt-3.5-turbo")),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"string"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Temperature"),(0,r.kt)("td",{parentName:"tr",align:null},'The sampling temperature to use. Lower values are more deterministic. Higher values are more "creative".'),(0,r.kt)("td",{parentName:"tr",align:null},"0.5"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"number"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Top P"),(0,r.kt)("td",{parentName:"tr",align:null},"Alternate sampling mode using the top X% of values. 0.1 corresponds to the top 10%."),(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"number"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Use Top P"),(0,r.kt)("td",{parentName:"tr",align:null},"Whether to use the Top P sampling mode."),(0,r.kt)("td",{parentName:"tr",align:null},"false"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"boolean"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Max Tokens"),(0,r.kt)("td",{parentName:"tr",align:null},"The maximum number of tokens that GPT is allowed to return. When hitting the max tokens, the response will be cut off."),(0,r.kt)("td",{parentName:"tr",align:null},"1024"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"number"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Stop"),(0,r.kt)("td",{parentName:"tr",align:null},"Comma separated list of stop tokens. If any stop token is encountered, the response will end immediately."),(0,r.kt)("td",{parentName:"tr",align:null},"(None)"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"string[]"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Presence Penalty"),(0,r.kt)("td",{parentName:"tr",align:null},"Applies a penalty or bonus for tokens that have already been used. See the OpenAI documentation for more info."),(0,r.kt)("td",{parentName:"tr",align:null},"0"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"number"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Frequency Penalty"),(0,r.kt)("td",{parentName:"tr",align:null},"Applies a penalty or bonus for tokens based on how much they have been used. See the OpenAI documentation for more info."),(0,r.kt)("td",{parentName:"tr",align:null},"0"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"number"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"User"),(0,r.kt)("td",{parentName:"tr",align:null},"Attaches a user field, for monitoring and detecting abuse. See the OpenAI documentation for more info."),(0,r.kt)("td",{parentName:"tr",align:null},"(Empty)"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"string"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Number of Choices"),(0,r.kt)("td",{parentName:"tr",align:null},"Allows the Chat Node to output a list of responses simultaneously if the value is greater than 1. Each response will have a unique set of text (assuming the temperature is more than 0)"),(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"number"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Enable Function Use"),(0,r.kt)("td",{parentName:"tr",align:null},"Allows the use of functions in the prompt. Enables the ",(0,r.kt)("inlineCode",{parentName:"td"},"Functions")," input."),(0,r.kt)("td",{parentName:"tr",align:null},"false"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Cache"),(0,r.kt)("td",{parentName:"tr",align:null},"Caches the response locally in Rivet. If the chat node gets the exact same prompt again, it will return the cached response instead of making a new request to OpenAI."),(0,r.kt)("td",{parentName:"tr",align:null},"false"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Use for subgraph partial output"),(0,r.kt)("td",{parentName:"tr",align:null},"If enabled, the Chat Node will be used to generate partial output for subgraphs. This is only visual - the chat node's partial output will appear as the subgraph's partial output. Only enable if the chat node will be running exclusively from other chat nodes at the same time, to avoid problems."),(0,r.kt)("td",{parentName:"tr",align:null},"true"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},"N/A")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Endpoint"),(0,r.kt)("td",{parentName:"tr",align:null},"The endpoint to use for the request. For example, you can set the endpoint to ",(0,r.kt)("inlineCode",{parentName:"td"},"http://localhost:1234/v1/chat/completions")," for LM Studio. You may also set a global Chat Node endpoint int the Rivet Settings page."),(0,r.kt)("td",{parentName:"tr",align:null},"(empty string, uses the default endpoint configured in Settings, or OpenAI's chat completions endpoint if that is not set.)"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"string"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Custom Model"),(0,r.kt)("td",{parentName:"tr",align:null},"If you are not using OpenAI's GPT, you can set the ",(0,r.kt)("inlineCode",{parentName:"td"},"model")," parameter here, to a custom string. This parameter overrides the model defined above."),(0,r.kt)("td",{parentName:"tr",align:null},"(None)"),(0,r.kt)("td",{parentName:"tr",align:null},"No"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"string"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Headers"),(0,r.kt)("td",{parentName:"tr",align:null},"Allows you to set custom headers to send with the request."),(0,r.kt)("td",{parentName:"tr",align:null},"(None)"),(0,r.kt)("td",{parentName:"tr",align:null},"Yes"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"object"))))),(0,r.kt)("p",null,"For all settings, see the ",(0,r.kt)("a",{parentName:"p",href:"https://platform.openai.com/docs/api-reference/chat/create"},"OpenAI API documentation")," for more information."))),(0,r.kt)("h3",{id:"example-1-simple-response"},"Example 1: Simple Response"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Add a Chat node to your graph."),(0,r.kt)("li",{parentName:"ol"},"Add a text node and place your message to GPT inside the text node by opening its editor and replacing ",(0,r.kt)("inlineCode",{parentName:"li"},"{{input}}")," with your message."),(0,r.kt)("li",{parentName:"ol"},"Connect the output of the text node to the ",(0,r.kt)("inlineCode",{parentName:"li"},"Prompt")," input of the Chat node."),(0,r.kt)("li",{parentName:"ol"},"Run your graph. You will see the output of the Chat node at the bottom of the node.")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"Simple Response Example",src:n(9826).Z,width:"1196",height:"980"})),(0,r.kt)("h3",{id:"example-2-connecting-to-lm-studio"},"Example 2: Connecting to LM Studio"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Add a Chat node to your graph."),(0,r.kt)("li",{parentName:"ol"},"Add a text node and place your message to GPT inside the text node by opening its editor and replacing ",(0,r.kt)("inlineCode",{parentName:"li"},"{{input}}")," with your message."),(0,r.kt)("li",{parentName:"ol"},"Connect the output of the text node to the ",(0,r.kt)("inlineCode",{parentName:"li"},"Prompt")," input of the Chat node."),(0,r.kt)("li",{parentName:"ol"},"Set the ",(0,r.kt)("inlineCode",{parentName:"li"},"Endpoint")," setting to ",(0,r.kt)("inlineCode",{parentName:"li"},"http://localhost:1234/v1/chat/completions"),"."),(0,r.kt)("li",{parentName:"ol"},"Load your desired model into LM Studio."),(0,r.kt)("li",{parentName:"ol"},"Enable CORS in LM Studio Server Options."),(0,r.kt)("li",{parentName:"ol"},"Run your graph. You will see the output of the Chat node at the bottom of the node.")),(0,r.kt)("h2",{id:"error-handling"},"Error Handling"),(0,r.kt)("p",null,"If nothing is connected to the ",(0,r.kt)("inlineCode",{parentName:"p"},"Prompt")," input, the Chat node will error."),(0,r.kt)("p",null,"If the request to OpenAI fails due to rate-limiting, the Chat node will retry the request using a jittered exponential backoff algorithm. This retry will\nhappen for up to 5 minutes. If the request still fails after 5 minutes, the Chat node will error."),(0,r.kt)("admonition",{type:"caution"},(0,r.kt)("p",{parentName:"admonition"},"Be careful ",(0,r.kt)("a",{parentName:"p",href:"/docs/user-guide/splitting"},"splitting")," a Chat node too much that you run into rate limiting issues.")),(0,r.kt)("p",null,"If OpenAI returns a 500-level error (due to being overloaded or downtime, etc), the Chat node will retry in a similar manner."),(0,r.kt)("h2",{id:"faq"},"FAQ"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Q: What if I connect a different data type to the prompt or system prompt input?")),(0,r.kt)("p",null,"A: The value will be attempted to be converted into a string, which will turn into a ",(0,r.kt)("inlineCode",{parentName:"p"},"user"),' type chat messages. So for example a number 5 will turn into a user message "5".\nIf the value cannot be converted to a string, then it will be ignored for the list of prompt messages.'),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Q: What if an input is toggled on, but not connected?")),(0,r.kt)("p",null,"A: The value configured in the UI will be used instead."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Q: What if the system prompt is connected, but the prompt is not?")),(0,r.kt)("p",null,"A: The Chat Node will error. The prompt input is required. To send only a system prompt, you can use a ",(0,r.kt)("a",{parentName:"p",href:"/docs/node-reference/prompt"},"Prompt")," node to create a system-type prompt, and connect it to the ",(0,r.kt)("inlineCode",{parentName:"p"},"Prompt")," input."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Q: What if the system prompt is connected, and the prompt also contains a system prompt?")),(0,r.kt)("p",null,"A: Both system prompts will be sent. System prompts that are not the first message in a chain are undefined behavior in GPT. It may work, or it may act strangely. It may follow one or both of the system prompts."),(0,r.kt)("h2",{id:"see-also"},"See Also"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/node-reference/assemble-prompt"},"Assemble Prompt Node")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/node-reference/prompt"},"Prompt Node")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/docs/node-reference/text"},"Text Node")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://platform.openai.com/docs/api-reference/chat/create"},"OpenAI API Documentation"))))}h.isMDXComponent=!0},9826:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/chat-node-example-01-d02666baca10ab4097e383cda0b7f82c.png"},8729:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/chat-node-11b598a8541cf7764743cb4264102d30.png"}}]);