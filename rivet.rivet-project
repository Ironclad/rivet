version: 3
data:
  graphs:
    BCH2-JTaOfU7yrJ1GQRhL:
      metadata:
        description: ""
        id: BCH2-JTaOfU7yrJ1GQRhL
        name: Extract List Items
      nodes:
        0ciq8PeUozuVOAQ8D6lQ_:
          data: {}
          id: 0ciq8PeUozuVOAQ8D6lQ_
          outgoingConnections:
            - output->"Graph Output" N-LYZkuLW9qvIFobDpRo5/value
          title: If/Else
          type: ifElse
          visualData: 1387/387/125/18
        FClS4P3cvRs8mZHieN9VO:
          data:
            caseCount: 1
            cases:
              - \*
          id: FClS4P3cvRs8mZHieN9VO
          outgoingConnections:
            - case1->"Extract Regex" uscIqNzkwiFK1zWdxydCj/input
            - case1->"If/Else" 0ciq8PeUozuVOAQ8D6lQ_/if
          title: Match
          type: match
          visualData: 663/451/300/12
        JXxrnJnWCwYxJbnlgh_KM:
          data:
            dataType: string
            id: text
          id: JXxrnJnWCwYxJbnlgh_KM
          outgoingConnections:
            - data->"Match" FClS4P3cvRs8mZHieN9VO/input
          title: Graph Input
          type: graphInput
          visualData: 288/323/300/8
        MGGvYxorqqDFpvGmFLWmw:
          data:
            text: ""
          id: MGGvYxorqqDFpvGmFLWmw
          outgoingConnections:
            - output->"If/Else" 0ciq8PeUozuVOAQ8D6lQ_/false
          title: Text
          type: text
          visualData: 688/648/154/16
        N-LYZkuLW9qvIFobDpRo5:
          data:
            dataType: string[]
            id: items
          id: N-LYZkuLW9qvIFobDpRo5
          outgoingConnections: []
          title: Graph Output
          type: graphOutput
          visualData: 1625/402/300/14
        uscIqNzkwiFK1zWdxydCj:
          data:
            errorOnFailed: true
            regex: \* (.+)
            useRegexInput: false
          id: uscIqNzkwiFK1zWdxydCj
          outgoingConnections:
            - matches->"If/Else" 0ciq8PeUozuVOAQ8D6lQ_/true
          title: Extract Regex
          type: extractRegex
          visualData: 1035/383/250/13
    HXjZhpWO0hluMiDY6pneE:
      metadata:
        description: ""
        id: HXjZhpWO0hluMiDY6pneE
        name: Digest File
      nodes:
        -c9ebiNnhmYedQN8Wo8hw:
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: -c9ebiNnhmYedQN8Wo8hw
          outgoingConnections:
            - response->"Text" IYT_PxYTFx5P4zMzhOqn_/name
            - response->"Text" IYT_PxYTFx5P4zMzhOqn_/responses
          title: Chat
          type: chat
          visualData: 1785.4317276583695/362.6631476479146/200/52
        7VrEtnVAjfEeOfFVEBu_B:
          data:
            dataType: string
            id: digest
          id: 7VrEtnVAjfEeOfFVEBu_B
          outgoingConnections: []
          title: Graph Output
          type: graphOutput
          visualData: 3540.1001678580496/818.9951954575467/300/96
        BykrT115qLE_8iVjmBCAp:
          data:
            text: Please tell me what the responsibility for this file is with respect to
              other files. Be extremely thorough in your response. Basically, if
              another person asked you "what is the purpose of this file? What
              does it do?", this would be your detailed response.
          id: BykrT115qLE_8iVjmBCAp
          outgoingConnections:
            - output->"Assemble Prompt" iNgsHuW1d-8xDv1A21bXK/message3
            - output->"Chat" _Gs7cTnnvcCHLxF8W5mFD/message3
          title: Text
          type: text
          visualData: 971.7770959717168/1315.8146243045526/300/94
        CNa2GH7LRJIXnWGt366QH:
          data:
            graphId: BCH2-JTaOfU7yrJ1GQRhL
          id: CNa2GH7LRJIXnWGt366QH
          outgoingConnections:
            - items->"Text" f6u3NUOg5S__5gSn8MXUF/exportedTypes
          title: Extract List Items
          type: subGraph
          visualData: 2392.746502255515/387.9603279891064/300/59
        Dl9zJxgRRBk7hQ43xWhZv:
          data:
            text: >-
              

              Please give me a a list of the top level exported names/functions/etc from this part of the file. Put the items in bullet points.


              Example:


              * type Foo

              * type Bar
          id: Dl9zJxgRRBk7hQ43xWhZv
          outgoingConnections:
            - output->"Assemble Prompt" piQXWR9MhnC5iwPAK7zOC/message3
            - output->"Chat" -c9ebiNnhmYedQN8Wo8hw/message3
          title: Text
          type: text
          visualData: 1037.1953261535937/389.80109239577234/300/91
        H7yY7sJF7zumxm1akfDD5:
          data:
            text: |-
              Here is part {{index}}/{{count}} of a file named {{fileName}}:

              ```
              {{content}}
              ```
          id: H7yY7sJF7zumxm1akfDD5
          outgoingConnections:
            - output->"Assemble Prompt" ZBqKPDo-QOpXD-Xo0XXpD/message2
            - output->"Assemble Prompt" iNgsHuW1d-8xDv1A21bXK/message2
            - output->"Assemble Prompt" piQXWR9MhnC5iwPAK7zOC/message2
            - output->"Chat" -c9ebiNnhmYedQN8Wo8hw/message2
            - output->"Chat" _Gs7cTnnvcCHLxF8W5mFD/message2
            - output->"Chat" jB5sIeojQRHXqNPoMQ3hD/message2
          title: Text
          type: text
          visualData: 921.042358937134/607.8022923490608/300/50
        IYT_PxYTFx5P4zMzhOqn_:
          data:
            text: "{{responses}}"
          id: IYT_PxYTFx5P4zMzhOqn_
          outgoingConnections:
            - output->"Extract List Items" CNa2GH7LRJIXnWGt366QH/text
          title: Text
          type: text
          visualData: 2045.0325311729325/391.53579298738487/300/57
        MA5Uv_ITTzx9NQV08ugic:
          data:
            text: "{{responses}}"
          id: MA5Uv_ITTzx9NQV08ugic
          outgoingConnections:
            - output->"Text" f6u3NUOg5S__5gSn8MXUF/imports
          title: Text
          type: text
          visualData: 2011.055065642513/922.81832877346/300/80
        SrMnr7J2d7Buwtd0vl8OF:
          data:
            model: gpt-3.5-turbo
            numTokensPerChunk: 2048
            overlap: 0
            useModelInput: false
          id: SrMnr7J2d7Buwtd0vl8OF
          outgoingConnections:
            - chunks->"Text" H7yY7sJF7zumxm1akfDD5/content
            - count->"Text" H7yY7sJF7zumxm1akfDD5/count
            - indexes->"Text" H7yY7sJF7zumxm1akfDD5/index
          title: Chunk
          type: chunk
          visualData: 513.6527622586358/541.0860856359533/200/49
        ZBqKPDo-QOpXD-Xo0XXpD:
          data: {}
          id: ZBqKPDo-QOpXD-Xo0XXpD
          outgoingConnections:
            - prompt->"Chat" jB5sIeojQRHXqNPoMQ3hD/prompt
          title: Assemble Prompt
          type: assemblePrompt
          visualData: 1434.5456573857782/944.3831259770584/250/93
        ZghD25qGelXbZLU6VLwLm:
          data:
            promptText: You are a sophisticated AI tool for extracting and analyzing code
              for the purposes of summarizing/digesting a code file. Given a
              request, you give a direct response.
            type: system
            useTypeInput: false
          id: ZghD25qGelXbZLU6VLwLm
          outgoingConnections:
            - output->"Assemble Prompt" ZBqKPDo-QOpXD-Xo0XXpD/message1
            - output->"Assemble Prompt" iNgsHuW1d-8xDv1A21bXK/message1
            - output->"Assemble Prompt" piQXWR9MhnC5iwPAK7zOC/message1
            - output->"Chat" -c9ebiNnhmYedQN8Wo8hw/message1
            - output->"Chat" _Gs7cTnnvcCHLxF8W5mFD/message1
            - output->"Chat" jB5sIeojQRHXqNPoMQ3hD/message1
          title: Prompt
          type: prompt
          visualData: 331.1046122293854/209.09150971792653/null/42
        _Gs7cTnnvcCHLxF8W5mFD:
          data:
            cache: true
            frequencyPenalty: 0.2
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0.2
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: _Gs7cTnnvcCHLxF8W5mFD
          outgoingConnections:
            - response->"Text" qP77T0iicnT05aOyXu4bl/responses
          title: Chat
          type: chat
          visualData: 1787.0996057501923/1218.8364162908351/200/73
        cQlhtmjsV2OU-_F9vbLni:
          data:
            text: "{{fileName}}"
          id: cQlhtmjsV2OU-_F9vbLni
          outgoingConnections:
            - output->"Text" f6u3NUOg5S__5gSn8MXUF/fileName
          title: Text
          type: text
          visualData: 2237.7406238561566/643.7676374637542/141.47817312027655/68
        f6u3NUOg5S__5gSn8MXUF:
          data:
            text: |-
              Here is a summary of {{fileName}}:

              Exported Types:
              {{exportedTypes}}

              Imports From:
              {{imports}}

              Summary:

              """
              {{summary}}
              ""
          id: f6u3NUOg5S__5gSn8MXUF
          outgoingConnections:
            - output->"Graph Output" 7VrEtnVAjfEeOfFVEBu_B/value
          title: Text
          type: text
          visualData: 3159.6649042897557/663.832260816221/300/90
        iNgsHuW1d-8xDv1A21bXK:
          data: {}
          id: iNgsHuW1d-8xDv1A21bXK
          outgoingConnections:
            - prompt->"Chat" _Gs7cTnnvcCHLxF8W5mFD/prompt
          title: Assemble Prompt
          type: assemblePrompt
          visualData: 1427.8561981049036/1298.9244678634084/250/95
        jB5sIeojQRHXqNPoMQ3hD:
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: jB5sIeojQRHXqNPoMQ3hD
          outgoingConnections:
            - response->"Text" MA5Uv_ITTzx9NQV08ugic/responses
          title: Chat
          type: chat
          visualData: 1785.2088940836688/871.453829230769/200/62
        pVOBgjtUCeoRf7O_aUK5z:
          data:
            dataType: string
            defaultValue: Nodes.ts
            id: file_name
          id: pVOBgjtUCeoRf7O_aUK5z
          outgoingConnections:
            - data->"Text" H7yY7sJF7zumxm1akfDD5/fileName
            - data->"Text" cQlhtmjsV2OU-_F9vbLni/fileName
          title: Graph Input
          type: graphInput
          visualData: 467.8341151842628/843.852274140362/300/77
        pZFkwyQ1DNAh158xMFaOr:
          data:
            text: >-
              If there are any relative imports, please list the files that are
              being imported from. If there are no relative imports, say NO
              IMPORTS.


              Example:


              * ./File1.ts

              * ../File2.ts


              Another example:


              NO IMPORTS
          id: pZFkwyQ1DNAh158xMFaOr
          outgoingConnections:
            - output->"Assemble Prompt" ZBqKPDo-QOpXD-Xo0XXpD/message3
            - output->"Chat" jB5sIeojQRHXqNPoMQ3hD/message3
          title: Text
          type: text
          visualData: 985.2658327776197/899.6643074227177/300/92
        piQXWR9MhnC5iwPAK7zOC:
          data: {}
          id: piQXWR9MhnC5iwPAK7zOC
          outgoingConnections:
            - prompt->"Chat" -c9ebiNnhmYedQN8Wo8hw/prompt
          title: Assemble Prompt
          type: assemblePrompt
          visualData: 1470.2227735504423/275.4371978896052/250/null
        qP77T0iicnT05aOyXu4bl:
          data:
            text: "{{responses}}"
          id: qP77T0iicnT05aOyXu4bl
          outgoingConnections:
            - output->"Text" f6u3NUOg5S__5gSn8MXUF/summary
          title: Text
          type: text
          visualData: 2068.5571459636226/1283.7209368325432/300/74
        wGuXGa_VR2yGn5F3Gm-r7:
          data:
            dataType: string
            id: file_contents
            useDefaultValueInput: true
          id: wGuXGa_VR2yGn5F3Gm-r7
          outgoingConnections:
            - data->"Chunk" SrMnr7J2d7Buwtd0vl8OF/input
          title: Graph Input
          type: graphInput
          visualData: 141.6021328645315/556.4574325729797/300/78
        yQD0z-eBeqo0Hmb0yhZ05:
          data:
            text: >
              import { ChartNode } from './NodeBase';

              import { UserInputNode, UserInputNodeImpl } from './nodes/UserInputNode';

              import { NodeImpl } from './NodeImpl';

              import { TextNode, TextNodeImpl } from './nodes/TextNode';

              import { ChatNode, ChatNodeImpl } from './nodes/ChatNode';

              import { PromptNode, PromptNodeImpl } from './nodes/PromptNode';

              import { match } from 'ts-pattern';

              import { ExtractRegexNode, ExtractRegexNodeImpl } from './nodes/ExtractRegexNode';

              import { CodeNode, CodeNodeImpl } from './nodes/CodeNode';

              import { MatchNode, MatchNodeImpl } from './nodes/MatchNode';

              import { IfNode, IfNodeImpl } from './nodes/IfNode';

              import { ReadDirectoryNode, ReadDirectoryNodeImpl } from './nodes/ReadDirectoryNode';

              import { ReadFileNode, ReadFileNodeImpl } from './nodes/ReadFileNode';

              import { IfElseNode, IfElseNodeImpl } from './nodes/IfElseNode';

              import { ChunkNode, ChunkNodeImpl } from './nodes/ChunkNode';

              import { GraphInputNode, GraphInputNodeImpl } from './nodes/GraphInputNode';

              import { GraphOutputNode, GraphOutputNodeImpl } from './nodes/GraphOutputNode';

              import { SubGraphNode, SubGraphNodeImpl } from './nodes/SubGraphNode';

              import { ArrayNode, ArrayNodeImpl } from './nodes/ArrayNode';

              import { ExtractJsonNode, ExtractJsonNodeImpl } from './nodes/ExtractJsonNode';

              import { AssemblePromptNode, AssemblePromptNodeImpl } from './nodes/AssemblePromptNode';

              import { LoopControllerNode, LoopControllerNodeImpl } from './nodes/LoopControllerNode';


              export type Nodes =
                | UserInputNode
                | TextNode
                | ChatNode
                | PromptNode
                | ExtractRegexNode
                | CodeNode
                | MatchNode
                | IfNode
                | ReadDirectoryNode
                | ReadFileNode
                | IfElseNode
                | ChunkNode
                | GraphInputNode
                | GraphOutputNode
                | SubGraphNode
                | ArrayNode
                | ExtractJsonNode
                | AssemblePromptNode
                | LoopControllerNode;

              export * from './nodes/UserInputNode';

              export * from './nodes/TextNode';

              export * from './nodes/ChatNode';

              export * from './nodes/PromptNode';

              export * from './nodes/ExtractRegexNode';

              export * from './nodes/CodeNode';

              export * from './nodes/MatchNode';

              export * from './nodes/IfNode';

              export * from './nodes/ReadDirectoryNode';

              export * from './nodes/ReadFileNode';

              export * from './nodes/IfElseNode';

              export * from './nodes/ChunkNode';

              export * from './nodes/GraphInputNode';

              export * from './nodes/GraphOutputNode';

              export * from './nodes/SubGraphNode';

              export * from './nodes/ArrayNode';

              export * from './nodes/ExtractJsonNode';

              export * from './nodes/AssemblePromptNode';

              export * from './nodes/LoopControllerNode';


              export type NodeType = Nodes['type'];


              export const createNodeInstance = <T extends Nodes>(node: T): NodeImpl<ChartNode> => {
                return match(node as Nodes)
                  .with({ type: 'userInput' }, (node) => new UserInputNodeImpl(node))
                  .with({ type: 'text' }, (node) => new TextNodeImpl(node))
                  .with({ type: 'chat' }, (node) => new ChatNodeImpl(node))
                  .with({ type: 'prompt' }, (node) => new PromptNodeImpl(node))
                  .with({ type: 'extractRegex' }, (node) => new ExtractRegexNodeImpl(node))
                  .with({ type: 'code' }, (node) => new CodeNodeImpl(node))
                  .with({ type: 'match' }, (node) => new MatchNodeImpl(node))
                  .with({ type: 'if' }, (node) => new IfNodeImpl(node))
                  .with({ type: 'readDirectory' }, (node) => new ReadDirectoryNodeImpl(node))
                  .with({ type: 'readFile' }, (node) => new ReadFileNodeImpl(node))
                  .with({ type: 'ifElse' }, (node) => new IfElseNodeImpl(node))
                  .with({ type: 'chunk' }, (node) => new ChunkNodeImpl(node))
                  .with({ type: 'graphInput' }, (node) => new GraphInputNodeImpl(node))
                  .with({ type: 'graphOutput' }, (node) => new GraphOutputNodeImpl(node))
                  .with({ type: 'subGraph' }, (node) => new SubGraphNodeImpl(node))
                  .with({ type: 'array' }, (node) => new ArrayNodeImpl(node))
                  .with({ type: 'extractJson' }, (node) => new ExtractJsonNodeImpl(node))
                  .with({ type: 'assemblePrompt' }, (node) => new AssemblePromptNodeImpl(node))
                  .with({ type: 'loopController' }, (node) => new LoopControllerNodeImpl(node))
                  .exhaustive();
              };


              export function createUnknownNodeInstance(node: ChartNode): NodeImpl<ChartNode> {
                return createNodeInstance(node as Nodes);
              }


              export function nodeFactory(type: NodeType): Nodes {
                return match(type)
                  .with('userInput', () => UserInputNodeImpl.create())
                  .with('text', () => TextNodeImpl.create())
                  .with('chat', () => ChatNodeImpl.create())
                  .with('prompt', () => PromptNodeImpl.create())
                  .with('extractRegex', () => ExtractRegexNodeImpl.create())
                  .with('code', () => CodeNodeImpl.create())
                  .with('match', () => MatchNodeImpl.create())
                  .with('if', () => IfNodeImpl.create())
                  .with('readDirectory', () => ReadDirectoryNodeImpl.create())
                  .with('readFile', () => ReadFileNodeImpl.create())
                  .with('ifElse', () => IfElseNodeImpl.create())
                  .with('chunk', () => ChunkNodeImpl.create())
                  .with('graphInput', () => GraphInputNodeImpl.create())
                  .with('graphOutput', () => GraphOutputNodeImpl.create())
                  .with('subGraph', () => SubGraphNodeImpl.create())
                  .with('array', () => ArrayNodeImpl.create())
                  .with('extractJson', () => ExtractJsonNodeImpl.create())
                  .with('assemblePrompt', () => AssemblePromptNodeImpl.create())
                  .with('loopController', () => LoopControllerNodeImpl.create())
                  .exhaustive();
              }


              export const nodeDisplayName: Record<NodeType, string> = {
                userInput: 'User Input',
                text: 'Text',
                chat: 'Chat',
                prompt: 'Prompt',
                assemblePrompt: 'Assemble Prompt',
                extractRegex: 'Extract With Regex',
                extractJson: 'Extract JSON',
                code: 'Code',
                match: 'Match',
                if: 'If',
                ifElse: 'If/Else',
                loopController: 'Loop Controller',
                readDirectory: 'Read Directory',
                readFile: 'Read File',
                chunk: 'Chunk',
                graphInput: 'Graph Input',
                graphOutput: 'Graph Output',
                subGraph: 'Subgraph',
                array: 'Array',
              };
          id: yQD0z-eBeqo0Hmb0yhZ05
          outgoingConnections:
            - output->"Graph Input" wGuXGa_VR2yGn5F3Gm-r7/default
          title: Text
          type: text
          visualData: -252.53117819207165/401.97028781741056/300/97
    JcFUPKbbvOvBQYdvItenL:
      metadata:
        description: ""
        id: JcFUPKbbvOvBQYdvItenL
        name: Get Nodai File By File Name
      nodes:
        3K-EbZswm_EjMm7YvDp-8:
          data:
            errorOnFailed: false
            regex: ([a-zA-Z]+)
            useRegexInput: true
          id: 3K-EbZswm_EjMm7YvDp-8
          outgoingConnections:
            - output1->"Extract Regex" dC9Q17Nml5jtpt-VrG2E6/input
            - output1->"Text" pL1gfs7YyNnriOtL8t129/path
          title: Extract Regex
          type: extractRegex
          visualData: -260.64747296379056/576.2537087926187/250/29
        6AXzyyVMSyqo6LN36roAj:
          data:
            text: (.*{{value}}.*)
          id: 6AXzyyVMSyqo6LN36roAj
          outgoingConnections:
            - output->"Extract Regex" 3K-EbZswm_EjMm7YvDp-8/regex
          title: Text
          type: text
          visualData: -622.3074425333289/708.7566296572189/300/26
        8WYEygiw4rtsog1_vAhUN:
          data:
            dataType: string
            id: file_name
          id: 8WYEygiw4rtsog1_vAhUN
          outgoingConnections: []
          title: Graph Output
          type: graphOutput
          visualData: 510/665/300/84
        DPHvuC_nC6P9cHeMx6AfY:
          data:
            dataType: string
            id: abs_path
          id: DPHvuC_nC6P9cHeMx6AfY
          outgoingConnections: []
          title: Graph Output
          type: graphOutput
          visualData: 498/257/300/85
        IJx_hhz0xYQMQ6LKp58Ut:
          data:
            dataType: string
            id: file_contents
          id: IJx_hhz0xYQMQ6LKp58Ut
          outgoingConnections: []
          title: Graph Output
          type: graphOutput
          visualData: 518/453/300/82
        XhzNj4pzQ2LB31xvUOa38:
          data:
            errorOnMissingFile: false
            path: ""
            usePathInput: true
          id: XhzNj4pzQ2LB31xvUOa38
          outgoingConnections:
            - content->"Graph Output" IJx_hhz0xYQMQ6LKp58Ut/value
          title: Read File
          type: readFile
          visualData: 78.53696254114706/589.5330651481295/250/30
        dC9Q17Nml5jtpt-VrG2E6:
          data:
            errorOnFailed: false
            regex: ([^/]+$)
            useRegexInput: false
          id: dC9Q17Nml5jtpt-VrG2E6
          outgoingConnections:
            - output1->"Graph Output" 8WYEygiw4rtsog1_vAhUN/value
          title: Extract Regex
          type: extractRegex
          visualData: 75.95255224213457/757.5597475716329/250/34
        hnMBT-lVJPf62eBijBv23:
          data:
            graphId: d6Pgmz7n8qvXkaNF-2e9P
          id: hnMBT-lVJPf62eBijBv23
          outgoingConnections:
            - files->"Text" o49i8FBO13BUTSJ9VVpUp/paths
            - root_path->"Text" pL1gfs7YyNnriOtL8t129/root
          title: List Nodai Files
          type: subGraph
          visualData: -974/509/300/80
        o49i8FBO13BUTSJ9VVpUp:
          data:
            text: "{{paths}}"
          id: o49i8FBO13BUTSJ9VVpUp
          outgoingConnections:
            - output->"Extract Regex" 3K-EbZswm_EjMm7YvDp-8/input
          title: Text
          type: text
          visualData: -619.508171986833/504.0287905543141/300/28
        pL1gfs7YyNnriOtL8t129:
          data:
            text: "{{root}}/{{path}}"
          id: pL1gfs7YyNnriOtL8t129
          outgoingConnections:
            - output->"Graph Output" DPHvuC_nC6P9cHeMx6AfY/value
            - output->"Read File" XhzNj4pzQ2LB31xvUOa38/path
          title: Text
          type: text
          visualData: -91.24293197539083/370.6588599708293/300/78
        pS3t0dhKo4pndES52TC25:
          data:
            dataType: string
            defaultValue: GraphProcessor.ts
            id: fileMatch
          id: pS3t0dhKo4pndES52TC25
          outgoingConnections:
            - data->"Text" 6AXzyyVMSyqo6LN36roAj/value
          title: Graph Input
          type: graphInput
          visualData: -973.2451596049881/711.7486797972081/300/24
    KAnpnQcN7_M7jDCUmZ19e:
      metadata:
        description: ""
        id: KAnpnQcN7_M7jDCUmZ19e
        name: Untitled Graph
      nodes: {}
    KVOCarbcccMX-wOaypQ7L:
      metadata:
        description: ""
        id: KVOCarbcccMX-wOaypQ7L
        name: Ask User Questions
      nodes:
        0EuQ8ONijZa5voMTZGTps:
          data:
            promptText: >-
              I have the following question or request:


              """

              {{request}}

              """


              Please give a bulleted list of questions. You may ask anywhere from zero to five questions. Try to only ask a question if it is necessary for your fulfillment of the request.


              Here is an example of an answer:


              Questions:

              * Question 1?

              * Question 2?

              * Question 3?


              Another example (if you have enough context):


              Questions: NONE
            type: user
            useTypeInput: false
          id: 0EuQ8ONijZa5voMTZGTps
          outgoingConnections:
            - output->"Chat" G4wKhESWcRoSKT7DSsNdy/message1
          title: Prompt
          type: prompt
          visualData: -193.28978127029873/304.7484539029439/null/16
        1Qj53m9VECmnmTWByVo_d:
          data:
            prompt: This is an example question?
            useInput: true
          id: 1Qj53m9VECmnmTWByVo_d
          outgoingConnections:
            - questionsAndAnswers->"Text" vpmf3hIf-86GRlg3Bg-Lu/qa
          title: User Input
          type: userInput
          visualData: 948/385/250/5
        2QcutpULV4uH0CkPgA3ti:
          data:
            dataType: string
            id: output
          id: 2QcutpULV4uH0CkPgA3ti
          outgoingConnections: []
          title: Graph Output
          type: graphOutput
          visualData: 1634/380/300/8
        G4wKhESWcRoSKT7DSsNdy:
          data:
            cache: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0.5
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: G4wKhESWcRoSKT7DSsNdy
          outgoingConnections:
            - response->"Extract List Items" XrdweoN7N1ectKzjP3WHS/text
          title: Chat
          type: chat
          visualData: 339.06620729836334/457.8536805742646/200/17
        XrdweoN7N1ectKzjP3WHS:
          data:
            graphId: BCH2-JTaOfU7yrJ1GQRhL
          id: XrdweoN7N1ectKzjP3WHS
          outgoingConnections:
            - items->"User Input" 1Qj53m9VECmnmTWByVo_d/questions
          title: Extract List Items
          type: subGraph
          visualData: 597/385/300/3
        glCn_a3AsNTs25UyCMCiG:
          data:
            dataType: string
            id: AI Prompt
          id: glCn_a3AsNTs25UyCMCiG
          outgoingConnections:
            - data->"Prompt" 0EuQ8ONijZa5voMTZGTps/request
          title: Graph Input
          type: graphInput
          visualData: -560.0248699650894/345.73225810806554/300/15
        vpmf3hIf-86GRlg3Bg-Lu:
          data:
            text: "{{qa}}"
          id: vpmf3hIf-86GRlg3Bg-Lu
          outgoingConnections:
            - output->"Graph Output" 2QcutpULV4uH0CkPgA3ti/value
          title: Text
          type: text
          visualData: 1265/407/300/7
    LOt0W_XCiFbuqbRlbF1oM:
      metadata:
        description: ""
        id: LOt0W_XCiFbuqbRlbF1oM
        name: Split and summarize file
      nodes:
        67RewDBJlQUInF8WV3-GB:
          data:
            text: >
              import { max, range, uniqBy } from 'lodash-es';

              import { ControlFlowExcluded } from '../utils/symbols';

              import { DataValue, ArrayDataValue, AnyDataValue, StringDataValue, expectType, ScalarDataValue } from './DataValue';

              import { ChartNode, NodeConnection, NodeId, NodeInputDefinition, NodeOutputDefinition, PortId } from './NodeBase';

              import { NodeGraph } from './NodeGraph';

              import { NodeImpl, ProcessContext } from './NodeImpl';

              import { Nodes, createNodeInstance } from './Nodes';

              import { UserInputNode, UserInputNodeImpl } from './nodes/UserInputNode';


              export type NodeResults = Map<string, Record<PortId, DataValue>>;


              export type ProcessEvents = {
                onNodeStart?: (node: ChartNode, inputs: Record<PortId, DataValue>) => void;
                onNodeFinish?: (node: ChartNode, result: Record<PortId, DataValue>) => void;
                onNodeError?: (node: ChartNode, error: Error) => void;
                onNodeExcluded?: (node: ChartNode) => void;
                onUserInput?: (
                  userInputNodes: UserInputNode[],
                  inputs: Record<PortId, DataValue>[],
                ) => Promise<ArrayDataValue<StringDataValue>[]>;
                onPartialOutputs?: (node: ChartNode, outputs: Record<PortId, DataValue>, index: number) => void;
              };


              export class GraphProcessor {
                #graph: NodeGraph;
                #nodeInstances: Record<NodeId, NodeImpl<ChartNode>>;
                #connections: Record<NodeId, NodeConnection[]>;
                #definitions: Record<NodeId, { inputs: NodeInputDefinition[]; outputs: NodeOutputDefinition[] }>;

                constructor(graph: NodeGraph) {
                  this.#graph = graph;
                  this.#nodeInstances = {};
                  this.#connections = {};

                  // Create node instances and store them in a lookup table
                  for (const node of this.#graph.nodes) {
                    this.#nodeInstances[node.id] = createNodeInstance(node as Nodes);
                  }

                  // Store connections in a lookup table
                  for (const conn of this.#graph.connections) {
                    if (!this.#connections[conn.inputNodeId]) {
                      this.#connections[conn.inputNodeId] = [];
                    }
                    if (!this.#connections[conn.outputNodeId]) {
                      this.#connections[conn.outputNodeId] = [];
                    }
                    this.#connections[conn.inputNodeId]!.push(conn);
                    this.#connections[conn.outputNodeId]!.push(conn);
                  }

                  // Store input and output definitions in a lookup table
                  this.#definitions = {};
                  for (const node of this.#graph.nodes) {
                    this.#definitions[node.id] = {
                      inputs: this.#nodeInstances[node.id]!.getInputDefinitions(this.#connections[node.id]!),
                      outputs: this.#nodeInstances[node.id]!.getOutputDefinitions(this.#connections[node.id]!),
                    };
                  }
                }

                #nodeIsReady(node: ChartNode, visitedNodes: Set<unknown>, depth = 0): boolean {
                  return this.#allInputsVisited(node, visitedNodes);
                }

                #allInputsVisited(node: ChartNode, visitedNodes: Set<unknown>, depth = 0): boolean {
                  const connections = this.#connections[node.id];
                  return (
                    this.#definitions[node.id]!.inputs.every((input) => {
                      const connectionToInput = connections?.find(
                        (conn) => conn.inputId === input.id && conn.inputNodeId === node.id,
                      );

                      if (!input.required && !connectionToInput) {
                        return true;
                      }

                      if (!connectionToInput) {
                        return false;
                      }

                      return visitedNodes.has(connectionToInput.outputNodeId);
                    }) || this.#definitions[node.id]!.inputs.length === 0
                  );
                }

                async processGraph(context: ProcessContext, events: ProcessEvents = {}): Promise<Record<string, DataValue>> {
                  const outputNodes = this.#graph.nodes.filter((node) => this.#definitions[node.id]!.outputs.length === 0);

                  const nodeResults: NodeResults = new Map();

                  // Process nodes in topological order
                  const nodesToProcess = [...this.#graph.nodes];
                  const visitedNodes = new Set();

                  while (nodesToProcess.length > 0) {
                    const readyNodes = nodesToProcess.filter((node) => this.#nodeIsReady(node, visitedNodes));

                    if (readyNodes.length === 0) {
                      for (const erroredNode of nodesToProcess) {
                        events.onNodeError?.(
                          erroredNode,
                          new Error('There might be a cycle in the graph or an issue with input dependencies.'),
                        );
                      }
                      throw new Error('There might be a cycle in the graph or an issue with input dependencies.');
                    }

                    const userInputNodes = readyNodes.filter((node) => node.type === 'userInput') as UserInputNode[];
                    if (userInputNodes.length > 0 && events.onUserInput) {
                      try {
                        const validUserInputNodes: UserInputNode[] = [];
                        const userInputInputValues: Record<PortId, DataValue>[] = [];

                        for (const node of userInputNodes) {
                          const inputValues = this.#getInputValuesForNode(node, nodeResults);
                          if (this.#excludedDueToControlFlow(node, nodeResults, inputValues, events, visitedNodes)) {
                            continue;
                          }
                          validUserInputNodes.push(node);
                          userInputInputValues.push(inputValues);
                          events.onNodeStart?.(node, inputValues);
                        }

                        if (validUserInputNodes.length > 0) {
                          const userInputResults = await events.onUserInput(validUserInputNodes, userInputInputValues);
                          userInputResults.forEach((result, index) => {
                            const node = validUserInputNodes[index]!;
                            const outputValues = (this.#nodeInstances[node.id] as UserInputNodeImpl).getOutputValuesFromUserInput(
                              userInputInputValues[index]!,
                              result,
                            );
                            nodeResults.set(node.id, outputValues);
                            visitedNodes.add(node.id);
                            nodesToProcess.splice(nodesToProcess.indexOf(node), 1);
                            events.onNodeFinish?.(node, outputValues);
                          });
                          continue;
                        }
                      } catch (error) {
                        for (const node of userInputNodes) {
                          events.onNodeError?.(node, error as Error);
                        }
                        throw error;
                      }
                    }

                    await Promise.allSettled(
                      readyNodes.map(async (node) => {
                        await this.#processNode(node as Nodes, nodeResults, context, events, visitedNodes, nodesToProcess);
                      }),
                    );
                  }

                  // Collect output values
                  const outputValues = outputNodes.reduce((values, node) => {
                    values[node.id] = nodeResults.get(node.id);
                    return values;
                  }, {} as Record<string, any>);

                  return outputValues;
                }

                async #processNode(
                  node: Nodes,
                  nodeResults: NodeResults,
                  context: ProcessContext,
                  events: ProcessEvents,
                  visitedNodes: Set<unknown>,
                  nodesToProcess: ChartNode[],
                ) {
                  nodesToProcess.splice(nodesToProcess.indexOf(node), 1);

                  if (node.isSplitRun) {
                    await this.#processSplitRunNode(node, nodeResults, context, events, visitedNodes);
                  } else {
                    await this.#processNormalNode(node, nodeResults, context, events, visitedNodes);
                  }
                }

                async #processSplitRunNode(
                  node: ChartNode,
                  nodeResults: NodeResults,
                  context: ProcessContext,
                  events: ProcessEvents,
                  visitedNodes: Set<unknown>,
                ) {
                  const inputValues = this.#getInputValuesForNode(node, nodeResults);

                  if (this.#excludedDueToControlFlow(node, nodeResults, inputValues, events, visitedNodes)) {
                    return;
                  }

                  const splittingAmount = Math.min(
                    max(Object.values(inputValues).map((value) => (Array.isArray(value.value) ? value.value.length : 1))) ?? 1,
                    node.splitRunMax ?? 10,
                  );

                  events.onNodeStart?.(node, inputValues);

                  try {
                    const results: Record<PortId, DataValue>[] = [];

                    await Promise.all(
                      range(0, splittingAmount).map(async (i) => {
                        const inputs: Record<PortId, DataValue> = Object.fromEntries(
                          Object.entries(inputValues).map(([port, value]): [PortId, DataValue] => {
                            if (value.type.endsWith('[]')) {
                              const newType = value.type.slice(0, -2) as DataValue['type'];
                              const newValue: unknown = (value.value as unknown[])[i] ?? undefined;
                              return [port as PortId, { type: newType, value: newValue as any }];
                            } else {
                              return [port as PortId, value];
                            }
                          }),
                        );

                        try {
                          const output = await this.#processNodeWithInputData(node, context, inputs, i, events.onPartialOutputs);
                          results.push(output);
                        } catch (error) {
                          const errorInstance =
                            typeof error === 'object' && error instanceof Error
                              ? error
                              : new Error(error != null ? error.toString() : 'Unknown error');
                          events.onNodeError?.(node, errorInstance);
                          throw error;
                        }
                      }),
                    );

                    // Combine the parallel results into the final output

                    // Turn a Record<PortId, DataValue[]> into a Record<PortId, AnyArrayDataValue>
                    const aggregateResults = results.reduce((acc, result) => {
                      for (const [portId, value] of Object.entries(result)) {
                        acc[portId as PortId] ??= { type: (value.type + '[]') as DataValue['type'], value: [] } as DataValue;
                        (acc[portId as PortId] as ArrayDataValue<AnyDataValue>).value.push(value.value);
                      }
                      return acc;
                    }, {} as Record<PortId, DataValue>);

                    nodeResults.set(node.id, aggregateResults);
                    visitedNodes.add(node.id);
                    events.onNodeFinish?.(node, aggregateResults);
                  } catch (error) {
                    const errorInstance =
                      typeof error === 'object' && error instanceof Error
                        ? error
                        : new Error(error != null ? error.toString() : 'Unknown error');
                    events.onNodeError?.(node, errorInstance);
                    console.error(error);
                    throw error;
                  }
                }

                async #processNormalNode(
                  node: ChartNode,
                  nodeResults: NodeResults,
                  context: ProcessContext,
                  events: ProcessEvents,
                  visitedNodes: Set<unknown>,
                ) {
                  const inputValues = this.#getInputValuesForNode(node, nodeResults);

                  if (this.#excludedDueToControlFlow(node, nodeResults, inputValues, events, visitedNodes)) {
                    return;
                  }

                  events.onNodeStart?.(node, inputValues);

                  try {
                    const outputValues = await this.#processNodeWithInputData(node, context, inputValues, 0, events.onPartialOutputs);

                    nodeResults.set(node.id, outputValues);
                    visitedNodes.add(node.id);
                    events.onNodeFinish?.(node, outputValues);
                  } catch (error) {
                    const errorInstance =
                      typeof error === 'object' && error instanceof Error
                        ? error
                        : new Error(error != null ? error.toString() : 'Unknown error');
                    events.onNodeError?.(node, errorInstance);
                    throw error;
                  }
                }

                async #processNodeWithInputData(
                  node: ChartNode,
                  context: ProcessContext,
                  inputValues: Record<PortId, DataValue>,
                  index: number,
                  onPartialOutputs?: (node: ChartNode, partialOutputs: Record<PortId, DataValue>, index: number) => void,
                ) {
                  return await this.#nodeInstances[node.id]!.process(inputValues, context, (partialOutputs) =>
                    onPartialOutputs?.(node, partialOutputs, index),
                  );
                }

                #excludedDueToControlFlow(
                  node: ChartNode,
                  nodeResults: NodeResults,
                  inputValues: Record<PortId, DataValue>,
                  { onNodeExcluded }: { onNodeExcluded?: (node: ChartNode) => void },
                  visitedNodes: Set<unknown>,
                ) {
                  const inputValuesList = Object.values(inputValues);
                  const inputIsExcludedValue =
                    inputValuesList.length > 0 && inputValuesList.some((value) => value?.type === 'control-flow-excluded');

                  const inputConnections = this.#connections[node.id]?.filter((conn) => conn.inputNodeId === node.id) ?? [];
                  const outputNodes = inputConnections
                    .map((conn) => this.#graph.nodes.find((n) => n.id === conn.outputNodeId))
                    .filter((n) => n) as ChartNode[];

                  const anyOutputIsExcludedValue =
                    outputNodes.length > 0 &&
                    outputNodes.some((outputNode) => {
                      const outputValues = nodeResults.get(outputNode.id) ?? {};
                      if (outputValues[ControlFlowExcluded as unknown as PortId]) {
                        return true;
                      }
                      return false;
                    });

                  const allowedToConsumedExcludedValue = node.type === 'if' || node.type === 'ifElse';

                  if ((inputIsExcludedValue || anyOutputIsExcludedValue) && !allowedToConsumedExcludedValue) {
                    onNodeExcluded?.(node);
                    visitedNodes.add(node.id);
                    nodeResults.set(node.id, {
                      [ControlFlowExcluded as unknown as PortId]: { type: 'control-flow-excluded', value: undefined },
                    });
                    return true;
                  }

                  return false;
                }

                #getInputValuesForNode(node: ChartNode, nodeResults: NodeResults): Record<PortId, DataValue> {
                  const connections = this.#connections[node.id];
                  return this.#definitions[node.id]!.inputs.reduce((values, input) => {
                    if (!connections) {
                      return values;
                    }
                    const connection = connections.find((conn) => conn.inputId === input.id && conn.inputNodeId === node.id);
                    if (connection) {
                      const outputNode = this.#nodeInstances[connection.outputNodeId]!.chartNode;
                      const outputResult = nodeResults.get(outputNode.id)?.[connection.outputId];

                      values[input.id] = outputResult;
                    }
                    return values;
                  }, {} as Record<string, any>);
                }
              }
          id: 67RewDBJlQUInF8WV3-GB
          outgoingConnections:
            - output->"Chunk" RtTfDTcXP3eigkIWeAhh-/input
          title: Text
          type: text
          visualData: 437.4469649520634/314.3452826722073/300/null
        F_dKF3vDhO7Cr1c8mvaKP:
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 4096
            model: gpt-3.5-turbo
            presencePenalty: 0
            temperature: 0
            top_p: 1
            useMaxTokensInput: false
            useModelInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: F_dKF3vDhO7Cr1c8mvaKP
          outgoingConnections:
            - response->"Text" rBQqW1q3Qp07IV6R66kMF/summaries
            - response->"Text" rBQqW1q3Qp07IV6R66kMF/summary
          title: Chat
          type: chat
          visualData: 2795.150414098868/521.6459310985977/200/16
        Gnh145qZDCCV2B23I5qZQ:
          data:
            text: |-
              Chunk {{chunk}}/{{total}}:

              {{response}}
          id: Gnh145qZDCCV2B23I5qZQ
          outgoingConnections:
            - output->"Text" wzOfI8FoCi4YPH7SMqzYv/summaries
          title: Text
          type: text
          visualData: 2095.460140320323/531.2883282385967/300/14
        I2obFLvUBKGtYx8arwMeA:
          data:
            cache: true
            frequencyPenalty: 0.5
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0.5
            temperature: 0
            top_p: 1
            useMaxTokensInput: false
            useModelInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: I2obFLvUBKGtYx8arwMeA
          outgoingConnections:
            - response->"Text" Gnh145qZDCCV2B23I5qZQ/response
          title: Chat
          type: chat
          visualData: 1841.424761136081/546.4923363640078/200/13
        RtTfDTcXP3eigkIWeAhh-:
          data:
            model: gpt-4
            numTokensPerChunk: 512
            overlap: 10
            useModelInput: false
          id: RtTfDTcXP3eigkIWeAhh-
          outgoingConnections:
            - chunks->"Text" ZJiOQyn0wI8ifplV9LgQt/data
            - count->"Text" Gnh145qZDCCV2B23I5qZQ/total
            - count->"Text" ZJiOQyn0wI8ifplV9LgQt/total
            - indexes->"Text" Gnh145qZDCCV2B23I5qZQ/chunk
            - indexes->"Text" ZJiOQyn0wI8ifplV9LgQt/chunk
          title: Chunk
          type: chunk
          visualData: 840.3950226023861/508.5530350479365/550.1024107453622/1
        ZJiOQyn0wI8ifplV9LgQt:
          data:
            text: >-
              This is chunk {{chunk}}/{{total}} of the GraphProcessor.ts file.


              Please take notes about this part of the file. Things to mark down are libraries used, techniques used, variable and function names, algorithms, imports, exports especially, etc.


              Be extremely thorough and dense in your response. Do not use superfluous words.
               
              ```

              {{data}}

              ```
          id: ZJiOQyn0wI8ifplV9LgQt
          outgoingConnections:
            - output->"Chat" I2obFLvUBKGtYx8arwMeA/message1
          title: Text
          type: text
          visualData: 1486.823749346318/511.12600971449524/300/4
        _vXnvuuQa4FO01wqqD22k:
          data:
            frequencyPenalty: 0.5
            maxTokens: 1024
            model: gpt-4
            presencePenalty: 0.5
            stop: ""
            temperature: 0.3
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: _vXnvuuQa4FO01wqqD22k
          outgoingConnections: []
          title: Chat
          type: chat
          visualData: 3399.2982395184267/530.0224627304983/200/19
        rBQqW1q3Qp07IV6R66kMF:
          data:
            text: |-
              Here is a summary of a TypeScript file called GraphProcessor.ts:

              """
              {{summary}}
              """

              Can you recreate this file to the best of your ability?
          id: rBQqW1q3Qp07IV6R66kMF
          outgoingConnections:
            - output->"Chat" _vXnvuuQa4FO01wqqD22k/message1
          title: Text
          type: text
          visualData: 3045.751670546527/529.0821793023815/300/18
        wzOfI8FoCi4YPH7SMqzYv:
          data:
            text: >-
              Please remove any redundancies and superfluous data from this
              summary, including the chunk index, "this section", "this chunk",
              etc.


              {{summaries}}
          id: wzOfI8FoCi4YPH7SMqzYv
          outgoingConnections:
            - output->"Chat" F_dKF3vDhO7Cr1c8mvaKP/message1
          title: Text
          type: text
          visualData: 2440.4366570143184/529.7076073959739/300/15
    X1x17cpmF8RFGb4TYdGF_:
      metadata:
        description: ""
        id: X1x17cpmF8RFGb4TYdGF_
        name: Untitled Graph
      nodes:
        2cl8uVa3thrtBhIHCuaK1:
          data:
            path: $.yamlDocument.name
            usePathInput: false
          id: 2cl8uVa3thrtBhIHCuaK1
          outgoingConnections: []
          title: Extract Object Path
          type: extractObjectPath
          visualData: 1634/519/250/10
        GgxznpUbo4lmVXSmWBjtC:
          data:
            text: >-
              Please give me some YAML about a user so that I can test
              extracting it from your message. The root property must be
              yamlDocument, like this:


              yamlDocument:
                key: value
          id: GgxznpUbo4lmVXSmWBjtC
          outgoingConnections:
            - output->"Chat" LT7zZ_80U7YMH4AJFNVrI/prompt
          title: Text
          type: text
          visualData: 717/547/300/1
        LT7zZ_80U7YMH4AJFNVrI:
          data:
            cache: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0.5
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: LT7zZ_80U7YMH4AJFNVrI
          outgoingConnections:
            - response->"Extract YAML" WiAcAGKAgBc2Hh18D7z0k/input
          title: Chat
          type: chat
          visualData: 1067/508/200/2
        WiAcAGKAgBc2Hh18D7z0k:
          data:
            rootPropertyName: yamlDocument
          id: WiAcAGKAgBc2Hh18D7z0k
          outgoingConnections:
            - output->"Extract Object Path" 2cl8uVa3thrtBhIHCuaK1/object
          title: Extract YAML
          type: extractYaml
          visualData: 1313/524/250/3
    d6Pgmz7n8qvXkaNF-2e9P:
      metadata:
        description: ""
        id: d6Pgmz7n8qvXkaNF-2e9P
        name: List Nodai Files
      nodes:
        FSHys4NrHcjHQvSE_Rv7-:
          data:
            dataType: string[]
            id: files
          id: FSHys4NrHcjHQvSE_Rv7-
          outgoingConnections: []
          title: Graph Output
          type: graphOutput
          visualData: -553/644/300/24
        oz0gemjC0VdsTeemnG_Kp:
          data:
            dataType: string
            id: root_path
          id: oz0gemjC0VdsTeemnG_Kp
          outgoingConnections: []
          title: Graph Output
          type: graphOutput
          visualData: -556/443/300/25
        wNG3QGg-NzvkTyoaynBz_:
          data:
            filterGlobs:
              - "**/*.{ts,tsx,css}"
            ignores:
              - "**/dist/**"
              - "**/src-tauri/**"
            includeDirectories: false
            path: /Users/andy.brenneke/Documents/nodai/packages
            recursive: true
            relative: true
            useFilterGlobsInput: false
            useIncludeDirectoriesInput: false
            usePathInput: false
            useRecursiveInput: false
            useRelativeInput: false
          id: wNG3QGg-NzvkTyoaynBz_
          outgoingConnections:
            - paths->"Graph Output" FSHys4NrHcjHQvSE_Rv7-/value
            - rootPath->"Graph Output" oz0gemjC0VdsTeemnG_Kp/value
          title: Read Directory
          type: readDirectory
          visualData: -1109.8933869703742/414.52061856748116/null/23
    trryHWyYAN99_ADAMDalG:
      metadata:
        description: ""
        id: trryHWyYAN99_ADAMDalG
        name: WorkflowServiceImpl Explanation
      nodes:
        0kYaxftKW7nshQnUHeleh:
          data:
            errorOnMissingFile: false
            path: /Users/Shared/ironclad/ironclad/harbor/packages/leaf-app-server/src/workflow/service/WorkflowServiceImpl.ts
            usePathInput: false
          id: 0kYaxftKW7nshQnUHeleh
          outgoingConnections:
            - content->"Chunk" XOjUzZkbpGh1GYBqidg97/input
          title: Read File
          type: readFile
          visualData: 806/161/250/2
        DZMA6R4Vz0t4qm8oS-2kI:
          data:
            cache: true
            frequencyPenalty: 0.5
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0.5
            stop: ""
            temperature: 0.2
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: DZMA6R4Vz0t4qm8oS-2kI
          outgoingConnections:
            - response->"Text" VirQr-vQN4fmaLvpsAqau/summary
          title: Chat
          type: chat
          visualData: 1985.8482769223158/25.624945733864607/200/20
        VirQr-vQN4fmaLvpsAqau:
          data:
            text: >-
              Hi, here's a summary of a file called WorkflowServiceImpl. Can you
              please explain to me the main algorithm used in this file?


              {{summary}}
          id: VirQr-vQN4fmaLvpsAqau
          outgoingConnections:
            - output->"Chat" xwRypu9DS55_4C1_goBE_/message1
          title: Text
          type: text
          visualData: 2272.205019111933/90.81184639491566/300/23
        WLRmmhSopqgZ44Z3AFO-9:
          data:
            promptText: You are a helpful code summarizer AI. You try to summarize code
              using technical details and avoid superfluous text.
            type: system
            useTypeInput: false
          id: WLRmmhSopqgZ44Z3AFO-9
          outgoingConnections:
            - output->"Chat" DZMA6R4Vz0t4qm8oS-2kI/message1
          title: Prompt
          type: prompt
          visualData: 1409.6426371505256/-70.99135346019317/null/18
        XOjUzZkbpGh1GYBqidg97:
          data:
            model: gpt-3.5-turbo
            numTokensPerChunk: 2048
            overlap: 10
            useModelInput: false
          id: XOjUzZkbpGh1GYBqidg97
          outgoingConnections:
            - chunks->"Prompt" wBNkA_KnnTKfE-ollR7ot/chunk
            - count->"Prompt" wBNkA_KnnTKfE-ollR7ot/total
            - indexes->"Prompt" wBNkA_KnnTKfE-ollR7ot/index
          title: Chunk
          type: chunk
          visualData: 1139.5826201261716/179.27978300634197/200/4
        wBNkA_KnnTKfE-ollR7ot:
          data:
            promptText: >-
              Hi, this is chunk {{index}}/{{total}} for the file called
              WorkflowServiceImpl. Can you please summarize what this part of
              the file does? Try to include technical details mostly.


              ```

              {{chunk}}

              ```
            type: user
            useTypeInput: false
          id: wBNkA_KnnTKfE-ollR7ot
          outgoingConnections:
            - output->"Chat" DZMA6R4Vz0t4qm8oS-2kI/message2
          title: Prompt
          type: prompt
          visualData: 1408.4785853530072/101.28831257258456/null/12
        xwRypu9DS55_4C1_goBE_:
          data:
            cache: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-4
            presencePenalty: 0
            stop: ""
            temperature: 0.3
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: xwRypu9DS55_4C1_goBE_
          outgoingConnections: []
          title: Chat
          type: chat
          visualData: 2623.748661962601/86.15563920484055/200/25
    yoe9VPnjUULQacHSolgiL:
      metadata:
        description: ""
        id: yoe9VPnjUULQacHSolgiL
        name: Execute Task List
      nodes:
        0bSFyrwELYC6bRERr5Znc:
          data:
            path: $.yamlDocument.remainingTasks[*]
            usePathInput: false
          id: 0bSFyrwELYC6bRERr5Znc
          outgoingConnections:
            - all_matches->"Remaining Task" mOqA0ixaGWn4vW2dVbGu7/input
          title: Remaining Tasks
          type: extractObjectPath
          visualData: 8858.792221881899/-10.430405764192535/250/302
        0xxgB-83ezLpsw0rLpo86:
          data:
            graphId: JcFUPKbbvOvBQYdvItenL
          id: 0xxgB-83ezLpsw0rLpo86
          outgoingConnections:
            - file_contents->"Text" i4cBHxWO1HLsnHg3Yh7qP/file_contents
          title: Get Nodai File By File Name
          type: subGraph
          visualData: 5887.468401346239/-209.5399625226982/300/269
        16UsT_tJlpYi717REkv6n:
          data: {}
          id: 16UsT_tJlpYi717REkv6n
          outgoingConnections:
            - prompt->"Chat" F3cvl2CAcA0PAj6KEiF6q/prompt
          title: Assemble Prompt
          type: assemblePrompt
          visualData: 1158/478/250/11
        18WP87xc4D7JXdSGTRlYD:
          data:
            dataType: string
            id: context
            useDefaultValueInput: true
          id: 18WP87xc4D7JXdSGTRlYD
          outgoingConnections:
            - data->"Execute Current Task Prompt" KIFBcqmOa5G-MPklHfHVR/context
            - data->"Text" uEWXLi8XmKpaiGzzTFDme/context
          title: Graph Input
          type: graphInput
          visualData: 2071.0798553565564/161.51608679287446/300/328
        2duN-SQvGKbhGgvdS0U6n:
          data:
            text: >-
              Here is a task list provided by another AI:


              """

              {{task_list}}

              """


              Your current task is: No task


              If your current task is complete, what is your next task? Please reply with a short key or sentence describing your next task to execute. 


              Reply in this format:


              Next Task: A short description of what I will be doing next
          id: 2duN-SQvGKbhGgvdS0U6n
          outgoingConnections:
            - output->"Assemble Prompt" 16UsT_tJlpYi717REkv6n/message2
          title: Text
          type: text
          visualData: 779.0555310294967/440.4303654785995/300/39
        3bvstHODyToviHmuGwEl0:
          data:
            text: "- {{input}}"
          id: 3bvstHODyToviHmuGwEl0
          outgoingConnections:
            - output->"Task List" tgcfyWHl6ut3-ivyZ8jEA/input
          title: Text
          type: text
          visualData: 864.2677686086065/869.4863033502647/132.05028524130944/null
        3uXtY4n-rlV41w6joxUdE:
          data:
            path: $.message
            usePathInput: false
          id: 3uXtY4n-rlV41w6joxUdE
          outgoingConnections:
            - match->"User Input" RIiW55iFfMEKP4fXdsETJ/questions
          title: Extract Object Path
          type: extractObjectPath
          visualData: 5570.698610051417/-729.7916844518388/250/355
        3xo9qKcGwicvWvyQkgZD4:
          data:
            rootPropertyName: yamlDocument
          id: 3xo9qKcGwicvWvyQkgZD4
          outgoingConnections:
            - output->"Next Task" xP7TQ0W3vIsCOyGtZEX5q/object
            - output->"Remaining Tasks" 0bSFyrwELYC6bRERr5Znc/object
          title: Extract YAML
          type: extractYaml
          visualData: 8587.35236611393/-174.5117304558507/134.44348359916876/299
        4CgGTnsYZdZFKi12cKmiP:
          data: {}
          id: 4CgGTnsYZdZFKi12cKmiP
          outgoingConnections:
            - output->"Text" FyLqlQhLYBHb5SD8Bbjx5/input
          title: If
          type: if
          visualData: 5752.306145236096/217.18316729256162/100/362
        4JUAkBJxsbKfNAXOdmGS6:
          data:
            path: $.notes
            usePathInput: false
          id: 4JUAkBJxsbKfNAXOdmGS6
          outgoingConnections:
            - match->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input7
          title: Extract Object Path
          type: extractObjectPath
          visualData: 5874.8418618563355/7.449881473971996/250/377
        4S1D75ofp0W1obD11VBqI:
          data:
            graphId: d6Pgmz7n8qvXkaNF-2e9P
          id: 4S1D75ofp0W1obD11VBqI
          outgoingConnections:
            - files->"Execute Current Task Prompt" KIFBcqmOa5G-MPklHfHVR/files
          title: List Nodai Files
          type: subGraph
          visualData: 3119.481362252188/-57.27713878661959/300/260
        6P04DpTqBQMOYpBfJuurJ:
          data:
            text: |-
              
              SYSTEM_MESSAGE:

              You ran this command:

              ```yaml
              command: {{command_name}}
              arguments: {{arguments}}
              ```

              The output of this command is:

              COMMAND_OUTPUT_START
              {{command_output}}
              COMMAND_OUTPUT_END
          id: 6P04DpTqBQMOYpBfJuurJ
          outgoingConnections:
            - output->"Command History" JA8Lk0pXQTMaNfUhEphm2/command
            - output->"Text" uEWXLi8XmKpaiGzzTFDme/command_info
          title: Text
          type: text
          visualData: 7486.925742104204/-160.8828855686792/300/229
        8xAF_gsGq9anwwpehGo2Y:
          data:
            text: >-
              You have Chat API functions available to you to use. You may call
              one of these functions to get a response from the system:


              - command: READ_FILES
                arguments:
                  files:
                    - file_name.ts
                description: Reads a file and returns with its contents. Files must be an array with one file per line.
              - command: TAKE_NOTE_FOR_SELF
                arguments:
                  notes: Here is some note to take for youself.
              - command: THINK_OUT_LOUD
                arguments:
                  notes: Here is some text to remember for later.
              - command: WRITE_FILE
                arguments:
                  file: file_name.ts
                contents: |
                  contents of the file
                description: Writes text to the specified file. You must write the entire file contents at once. You must not comment out sections of the file - everything needs to be implemented.
              - command: ASK_FOR_FEEDBACK
                arguments:
                  message: The message for the user
                description: Asks the user questions or for feedback on what you are doing. This can help make sure your plan is good.

              An example is:


              ```yaml

              yamlDocument:
                command: THINK_OUT_LOUD
                arguments:
                  notes: This is me thinking out loud
              ```
          id: 8xAF_gsGq9anwwpehGo2Y
          outgoingConnections:
            - output->"Execute Current Task Prompt"
              KIFBcqmOa5G-MPklHfHVR/functions
            - output->"Text" uEWXLi8XmKpaiGzzTFDme/functions
          title: Text
          type: text
          visualData: 2432.601813980837/872.2598561008039/300/160
        B0u3WpTZggnq8Lgl7Xv8h:
          data:
            text: >-
              Here is some text about a task list:


              """

              {{text}}

              """


              Convert this text into a YAML document with the following format:


              ```yaml

              yamlDocument:
                nextTask: The next task I should execute
                remainingTasks:
                  - The next task remaining 
                  - Another task remaining
              ``` 

              Every task MUST be a string value, even if the step contains YAML data.
          id: B0u3WpTZggnq8Lgl7Xv8h
          outgoingConnections:
            - output->"Chat" PLU5yC3u52dOPIxlmPIf-/prompt
          title: Text
          type: text
          visualData: 8226.95339201243/-548.2113674897184/300/290
        CGAZhTY_eFuZpMhTT_-XX:
          data:
            text: You are a AI tool that analyzes another AI's message and determines the
              next task for it to run.
          id: CGAZhTY_eFuZpMhTT_-XX
          outgoingConnections:
            - output->"Chat" gHSSvkoWQ76ko-0EvUdZn/systemPrompt
          title: Text
          type: text
          visualData: 7832.703318346902/-435.1156370653609/300/289
        CWz3hh4xEp4FLbYV3Tw3v:
          data: {}
          id: CWz3hh4xEp4FLbYV3Tw3v
          outgoingConnections:
            - output->"Extract Object Path" wq5E83vpWqQMlS7AHPnQe/object
          title: If
          type: if
          visualData: 5388.238498672117/-404.8323310274315/100/346
        F3cvl2CAcA0PAj6KEiF6q:
          data:
            cache: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0.5
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: F3cvl2CAcA0PAj6KEiF6q
          outgoingConnections:
            - response->"Extract Regex" L8j7UVv6-Qc2Pnm9B4MRB/input
          title: Chat
          type: chat
          visualData: 1478/413/200/13
        FyLqlQhLYBHb5SD8Bbjx5:
          data:
            text: |-
              SYSTEM ERROR: NO SUCH FUNCTION

              INPUT: 

              ```yml
              {{input}}
              ```
          id: FyLqlQhLYBHb5SD8Bbjx5
          outgoingConnections:
            - output->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input6
          title: Text
          type: text
          visualData: 6421.102213946726/101.64363655647925/300/374
        H4TGM34XgUZZOny6PfmFt:
          data:
            text: "{{input}}"
          id: H4TGM34XgUZZOny6PfmFt
          outgoingConnections:
            - output->"Loop Controller" cObRObIxgwrJablQHLGnw/input2
          title: Remaining Tasks
          type: text
          visualData: 9376.061220068554/4.762078225718085/154.63565140719766/326
        ItFByM3wvP_dPEoP5D6Mx:
          data: {}
          id: ItFByM3wvP_dPEoP5D6Mx
          outgoingConnections:
            - output->"Coalesce" YJDAqTtxqpNkKPX5FvtPt/input2
          title: If
          type: if
          visualData: 5380.482613085664/157.3401052405825/100/373
        JA8Lk0pXQTMaNfUhEphm2:
          data:
            text: |-
              {{prev_commands}}
              {{command}}
          id: JA8Lk0pXQTMaNfUhEphm2
          outgoingConnections:
            - output->"Loop Controller" cObRObIxgwrJablQHLGnw/input3
          title: Command History
          type: text
          visualData: 7911.318384602866/322.2310779162274/300/382
        KIFBcqmOa5G-MPklHfHVR:
          data:
            text: >-
              {{context}}


              Here are all files in the project:


              FILES_START

              {{files}}

              FILES_END


              {{functions}}


              Here is a history of your commands:


              HISTORY_START

              {{previously_executed}}

              HISTORY_END


              Here are your remaining tasks:


              - {{current_task}}

              {{task_list}}


              Reply with the Chat API function you will run next to further your task list. Primarily, focus on executing the first task(s) in your list. Do not run commands that have already ran before.



              Reply with a YAML document explaining your command, like this:


              ```yaml

              yamlDocument:
                currentTask: Restate your current task from the task list
                reasonForCommand: Explain the reason for choosing the command and how it will help execute your task list.
                command: COMMAND_NAME
                arguments:
                  argumentName: value
                  argument2: value2
              ```
          id: KIFBcqmOa5G-MPklHfHVR
          outgoingConnections:
            - output->"Chat - Execute Current Task" NlqmTc5SUrwdCFOOU26CP/prompt
          title: Execute Current Task Prompt
          type: text
          visualData: 3827.4091396369313/31.86885376256144/300/215
        L27NwFvq6cJgke83F55ei:
          data:
            prompt: This is an example question?
            useInput: true
          id: L27NwFvq6cJgke83F55ei
          outgoingConnections:
            - output->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input2
          title: User Input
          type: userInput
          visualData: 6271.2758460845735/-1025.009768641542/250/254
        L8j7UVv6-Qc2Pnm9B4MRB:
          data:
            errorOnFailed: false
            regex: "Next Task: (.+)"
            useRegexInput: false
          id: L8j7UVv6-Qc2Pnm9B4MRB
          outgoingConnections:
            - output1->"Loop Controller" cObRObIxgwrJablQHLGnw/input1Default
          title: Extract Regex
          type: extractRegex
          visualData: 1734/445/145/22
        MHz_Mr-O-KivwLl_Qbevp:
          data:
            text: >-
              yamlDocument:
                steps:
                  - First, I will review the requirements and specifications for the TrimChatMessagesNode component to ensure that I understand what is expected of me.
                  - Next, I will review the existing codebase to gain an understanding of how the system works and how the TrimChatMessagesNode component fits into it.
                  - I will examine the ExtractRegexNode.tsx file to see how a similar component is implemented and determine if there are any useful patterns or techniques that can be applied to the TrimChatMessagesNode component.
                  - I will create a new file called TrimChatMessagesNode.tsx and implement the components according to the specifications and examples provided.
          id: MHz_Mr-O-KivwLl_Qbevp
          outgoingConnections:
            - output->"Extract YAML" bjL5VE8Zzz84m3QFLJ7x7/input
          title: Text
          type: text
          visualData: -870.9610414764056/564.0734459241355/300/274
        Mvbq2URURhELcYZH2EUKw:
          data: {}
          id: Mvbq2URURhELcYZH2EUKw
          outgoingConnections:
            - output->"Extract Object Path" 3uXtY4n-rlV41w6joxUdE/object
          title: If
          type: if
          visualData: 5375.549822945474/-705.1813531485337/100/351
        NlqmTc5SUrwdCFOOU26CP:
          data:
            cache: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-4
            presencePenalty: 0
            stop: ""
            temperature: 0.2
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: NlqmTc5SUrwdCFOOU26CP
          outgoingConnections:
            - response->"Extract YAML" RRQkzAYP86x8a9_9_KOx_/input
            - response->"If" 4CgGTnsYZdZFKi12cKmiP/value
          title: Chat - Execute Current Task
          type: chat
          visualData: 4196.338904061931/191.56471881852312/194.08208299619673/278
        PLU5yC3u52dOPIxlmPIf-:
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: PLU5yC3u52dOPIxlmPIf-
          outgoingConnections:
            - response->"Extract YAML" 3xo9qKcGwicvWvyQkgZD4/input
          title: Chat
          type: chat
          visualData: 8606.039472972514/-393.373390759543/200/292
        Q2-bYh6LGkVm2WPb46tEz:
          data:
            text: "Please create this file: {{file_path}}"
          id: Q2-bYh6LGkVm2WPb46tEz
          outgoingConnections:
            - output->"User Input" L27NwFvq6cJgke83F55ei/questions
          title: Text
          type: text
          visualData: 5885.144058035461/-1031.3397979538224/300/251
        RIiW55iFfMEKP4fXdsETJ:
          data:
            prompt: This is an example question?
            useInput: true
          id: RIiW55iFfMEKP4fXdsETJ
          outgoingConnections:
            - output->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input3
          title: User Input
          type: userInput
          visualData: 5907.5865905173505/-741.4244278640571/250/259
        RNxCaWdA7mOcrJBVUKiqr:
          data: {}
          id: RNxCaWdA7mOcrJBVUKiqr
          outgoingConnections:
            - output->"Extract Object Path" iAPKowmhorGhT6DKcjkb7/object
          title: If
          type: if
          visualData: 5389.610793391964/-187.54344359294475/100/368
        RRQkzAYP86x8a9_9_KOx_:
          data:
            objectPath: $.yamlDocument
            rootPropertyName: yamlDocument
          id: RRQkzAYP86x8a9_9_KOx_
          outgoingConnections:
            - output->"Extract Object Path" TflHhIRIPHK_PlpMpMCtL/object
            - output->"Extract Object Path" z5IaklOOBA20wwMI0aRkq/object
          title: Extract YAML
          type: extractYaml
          visualData: 4198.2280237908735/436.0668212402672/149.28242186462194/281
        Rh5FbKt-FMnow6je0Hm4u:
          data: {}
          id: Rh5FbKt-FMnow6je0Hm4u
          outgoingConnections:
            - output->"Extract Object Path" u2sddIBdrSCM-8EkJ_qS4/object
          title: If
          type: if
          visualData: 5391.813974291001/-998.058677960689/100/358
        SNJjOp-rPmQwnU52nE8kO:
          data: {}
          id: SNJjOp-rPmQwnU52nE8kO
          outgoingConnections:
            - output->"Text" 6P04DpTqBQMOYpBfJuurJ/command_output
          title: Coalesce
          type: coalesce
          visualData: 7107.0327313666685/-155.45997396385604/150/285
        SNQCA2ZXN7LQRbPd1M6Mx:
          data:
            graphId: JcFUPKbbvOvBQYdvItenL
          id: SNQCA2ZXN7LQRbPd1M6Mx
          outgoingConnections:
            - file_contents->"Digest File" cV8OdlEuJDaI2v05bTz5X/file_contents
          title: Get Nodai File By File Name
          type: subGraph
          visualData: 5899.625587751349/-459.07944232089864/300/267
        TGIDciqmI5zMTLdVk2T2C:
          data: {}
          id: TGIDciqmI5zMTLdVk2T2C
          outgoingConnections:
            - output->"Coalesce" YJDAqTtxqpNkKPX5FvtPt/input1
          title: If
          type: if
          visualData: 5396.161154674258/9.417887736214652/100/367
        TflHhIRIPHK_PlpMpMCtL:
          data:
            path: $.command
            usePathInput: false
          id: TflHhIRIPHK_PlpMpMCtL
          outgoingConnections:
            - match->"Match" eKe_lJayfu9fVyZAD7ri_/input
            - match->"Text" 6P04DpTqBQMOYpBfJuurJ/command_name
          title: Extract Object Path
          type: extractObjectPath
          visualData: 4437.355919239172/355.562416243158/250/372
        U0EcCqP66WFZahvpiGPFf:
          data:
            prompt: This is an example question?
            useInput: true
          id: U0EcCqP66WFZahvpiGPFf
          outgoingConnections:
            - output->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input1
          title: User Input
          type: userInput
          visualData: 6281.8258949383735/-1270.8259069351022/250/256
        VIZaMd_lS9i-zMjpKl5wj:
          data:
            text: "{{input}}"
          id: VIZaMd_lS9i-zMjpKl5wj
          outgoingConnections:
            - output->"Text" uEWXLi8XmKpaiGzzTFDme/task_list
          title: Task List
          type: text
          visualData: 5995.575145528108/766.7859412690377/280.41157562862554/384
        Vudek72LbIFekPR5DVDsq:
          data:
            dataType: string[]
            id: task_list
            useDefaultValueInput: true
          id: Vudek72LbIFekPR5DVDsq
          outgoingConnections:
            - data->"Text" 2duN-SQvGKbhGgvdS0U6n/task_list
            - data->"Text" 3bvstHODyToviHmuGwEl0/input
          title: Task List
          type: graphInput
          visualData: 424/642/300/4
        YDjSbuZX0TA_4lFNEQGaN:
          data:
            text: You are an AI that converts English text into YAML documents, given a
              structure to output in.
          id: YDjSbuZX0TA_4lFNEQGaN
          outgoingConnections:
            - output->"Chat" PLU5yC3u52dOPIxlmPIf-/systemPrompt
          title: Text
          type: text
          visualData: 8217.342758974006/-728.677698989026/300/294
        YJDAqTtxqpNkKPX5FvtPt:
          data: {}
          id: YJDAqTtxqpNkKPX5FvtPt
          outgoingConnections:
            - output->"Extract Object Path" 4JUAkBJxsbKfNAXOdmGS6/object
          title: Coalesce
          type: coalesce
          visualData: 5664.504228276922/38.811399688325125/150/376
        YMPhS3MhQdfMqPqj8Ss8k:
          data:
            text: |-
              Please write these contents to {{file_path}}:

              {{contents}}
          id: YMPhS3MhQdfMqPqj8Ss8k
          outgoingConnections:
            - output->"User Input" U0EcCqP66WFZahvpiGPFf/questions
          title: Text
          type: text
          visualData: 5903.034048264701/-1281.4210486111251/300/255
        _tOy610lqFqHX6iUfQZu1:
          data:
            path: $.file
            usePathInput: false
          id: _tOy610lqFqHX6iUfQZu1
          outgoingConnections:
            - match->"Text" YMPhS3MhQdfMqPqj8Ss8k/file_path
          title: Extract Object Path
          type: extractObjectPath
          visualData: 5570.538237015827/-1264.1784150771389/250/361
        aymg3sOX3k4VaMI8L7NUg:
          data:
            text: >
              Here is additional context:


              I am working on developing an AI storyboarding tool that allows users to create a series of prompts for a language model in a choose-your-own-adventure format. The tool is inspired by node-based editors, like the one found in Blender, where users can create nodes on a page that have inputs and outputs that can be connected by wires to form a web of connections between the prompts and the AI. Each node can be edited, and when editing, a larger window pops up with a text editor where users can tweak various aspects of the prompt that will be fed to the AI. This tool will provide a user-friendly interface for crafting interactive stories with an AI language model. Here is a tree of my current files for context. If you would like the contents of any of these files, please ask. The app is dark-themed and the colors are available in index.css. I'm using Emotion for CSS.


              I have asked this question:


              """

              I need to implement the TrimChatMessagesNode.tsx file for the TrimChatMessages.ts file.

              """


              Here are some additional notes:


              Can you provide more information on the expected behavior of the TrimChatMessagesNode component?

              It takes in an chat-message[] and cuts messages off the beginning or end until it reaches a specified token count.

              Are there any specific styling requirements for this component?

              No

              How does the TrimChatMessagesNode component interact with the rest of the system?

              Same as everything else 

              Are there any existing tests for the TrimChatMessagesNode component that I should be aware of?

              No

              Is there any documentation or examples available for similar components that I can reference while implementing this?

              A good example would be ExtractRegexNode.ts
          id: aymg3sOX3k4VaMI8L7NUg
          outgoingConnections:
            - output->"Graph Input" 18WP87xc4D7JXdSGTRlYD/default
          title: Text
          type: text
          visualData: 1596.3955567814364/-106.05238101849785/300/35
        bjL5VE8Zzz84m3QFLJ7x7:
          data:
            rootPropertyName: yamlDocument
          id: bjL5VE8Zzz84m3QFLJ7x7
          outgoingConnections:
            - output->"Extract Object Path" qt2AdWZptLCfvACUeDtea/object
          title: Extract YAML
          type: extractYaml
          visualData: -511.4509344968344/638.2666631869166/250/275
        cER_B2LLcZYy15z-KNJDK:
          data:
            promptText: You are a programming assistant that iteratively executes commands
              and thinks out loud to accomplish tasks.
            type: system
            useTypeInput: false
          id: cER_B2LLcZYy15z-KNJDK
          outgoingConnections:
            - output->"Assemble Prompt" 16UsT_tJlpYi717REkv6n/message1
            - output->"System" z1APvC5m5fW-AIfzoVeox/input
          title: Prompt
          type: prompt
          visualData: 553.1915381834611/-17.88559476470175/null/28
        cObRObIxgwrJablQHLGnw:
          data: {}
          id: cObRObIxgwrJablQHLGnw
          outgoingConnections:
            - break->"Graph Output" xe4vk7BUOsT-mnIoEjOha/value
            - output1->"Execute Current Task Prompt"
              KIFBcqmOa5G-MPklHfHVR/current_task
            - output1->"Text" uEWXLi8XmKpaiGzzTFDme/current_task
            - output2->"Execute Current Task Prompt"
              KIFBcqmOa5G-MPklHfHVR/task_list
            - output2->"Task List" VIZaMd_lS9i-zMjpKl5wj/input
            - output3->"Command History" JA8Lk0pXQTMaNfUhEphm2/prev_commands
            - output3->"Execute Current Task Prompt"
              KIFBcqmOa5G-MPklHfHVR/previously_executed
          title: Loop Controller
          type: loopController
          visualData: 2012.432896034012/500.84246666707406/439.1093458908531/133
        cV8OdlEuJDaI2v05bTz5X:
          data:
            graphId: HXjZhpWO0hluMiDY6pneE
          id: cV8OdlEuJDaI2v05bTz5X
          outgoingConnections:
            - digest->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input4
          title: Digest File
          type: subGraph
          visualData: 6325.94927766123/-439.4589639215074/300/130
        dHDzOydr6Q5IFDBH6z6HU:
          data: {}
          id: dHDzOydr6Q5IFDBH6z6HU
          outgoingConnections:
            - output->"Extract Object Path" _tOy610lqFqHX6iUfQZu1/object
          title: If
          type: if
          visualData: 5368.851714764555/-1237.700159446597/100/359
        eKe_lJayfu9fVyZAD7ri_:
          data:
            caseCount: 7
            cases:
              - DIGEST_FILE
              - READ_FILES
              - TAKE_NOTE_FOR_SELF
              - ASK_FOR_FEEDBACK
              - CREATE_BLANK_FILE
              - WRITE_FILE
              - THINK_OUT_LOUD
          id: eKe_lJayfu9fVyZAD7ri_
          outgoingConnections:
            - case1->"If" CWz3hh4xEp4FLbYV3Tw3v/if
            - case2->"If" RNxCaWdA7mOcrJBVUKiqr/if
            - case3->"If" TGIDciqmI5zMTLdVk2T2C/if
            - case4->"If" Mvbq2URURhELcYZH2EUKw/if
            - case5->"If" Rh5FbKt-FMnow6je0Hm4u/if
            - case6->"If" dHDzOydr6Q5IFDBH6z6HU/if
            - case7->"If" ItFByM3wvP_dPEoP5D6Mx/if
            - unmatched->"If" 4CgGTnsYZdZFKi12cKmiP/if
          title: Match
          type: match
          visualData: 4151.319415255813/-535.8389204523792/300/334
        gHSSvkoWQ76ko-0EvUdZn:
          data:
            cache: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-4
            presencePenalty: 0
            stop: ""
            temperature: 0.3
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: gHSSvkoWQ76ko-0EvUdZn
          outgoingConnections:
            - response->"Text" B0u3WpTZggnq8Lgl7Xv8h/text
          title: Chat
          type: chat
          visualData: 8224.64735040648/-110.60521742736663/200/288
        i4cBHxWO1HLsnHg3Yh7qP:
          data:
            text: |-
              ```
              // {{file_name}}
              {{file_contents}}
              ```
          id: i4cBHxWO1HLsnHg3Yh7qP
          outgoingConnections:
            - output->"Coalesce" SNJjOp-rPmQwnU52nE8kO/input5
          title: Text
          type: text
          visualData: 6300.416322248281/-178.5223306236974/300/272
        iAPKowmhorGhT6DKcjkb7:
          data:
            path: $.files[*]
            usePathInput: false
          id: iAPKowmhorGhT6DKcjkb7
          outgoingConnections:
            - all_matches->"Get Nodai File By File Name"
              0xxgB-83ezLpsw0rLpo86/fileMatch
            - all_matches->"Text" i4cBHxWO1HLsnHg3Yh7qP/file_name
          title: Extract Object Path
          type: extractObjectPath
          visualData: 5557.238871649446/-193.98449142035096/250/337
        mOqA0ixaGWn4vW2dVbGu7:
          data:
            text: "- {{input}}"
          id: mOqA0ixaGWn4vW2dVbGu7
          outgoingConnections:
            - output->"Remaining Tasks" H4TGM34XgUZZOny6PfmFt/input
          title: Remaining Task
          type: text
          visualData: 9157.914625813317/9.440688943435534/176.6780763504812/315
        qt2AdWZptLCfvACUeDtea:
          data:
            path: $.yamlDocument.steps
            usePathInput: false
          id: qt2AdWZptLCfvACUeDtea
          outgoingConnections:
            - match->"Task List" Vudek72LbIFekPR5DVDsq/default
          title: Extract Object Path
          type: extractObjectPath
          visualData: -33.58278249236042/631.2095385792555/250/277
        tgcfyWHl6ut3-ivyZ8jEA:
          data:
            text: "{{input}}"
          id: tgcfyWHl6ut3-ivyZ8jEA
          outgoingConnections:
            - output->"Loop Controller" cObRObIxgwrJablQHLGnw/input2Default
          title: Task List
          type: text
          visualData: 1045.9677524118683/852.789548081857/116.3356920475137/370
        u2sddIBdrSCM-8EkJ_qS4:
          data:
            path: $.file
            usePathInput: false
          id: u2sddIBdrSCM-8EkJ_qS4
          outgoingConnections:
            - match->"Text" Q2-bYh6LGkVm2WPb46tEz/file_path
          title: Extract Object Path
          type: extractObjectPath
          visualData: 5550.538237015827/-1026.4048579184675/250/357
        uEWXLi8XmKpaiGzzTFDme:
          data:
            text: >-
              {{context}}


              {{functions}}


              Your current task was "{{current_task}}" and you ran this command to fulfill it:


              {{command_info}}


              Here are your remaining tasks:


              {{task_list}}


              Does your last command ran above finish your current task "{{current_task}}"? 


              List the tasks that your last command completed.


              Next, imagine a scenario where you are an expert in the Nodai project, would they consider your current task completed? Would they consider the remaining tasks still applicable? Here are some examples of what an expert might ask:


              "If you need examples, do you have enough? Or would more be better?"

              "If you have to guess about anything, would looking at any other files help?"

              "Have you taken enough notes for yourself?"

              "You could combine these steps"

              "This step sounds redundant"


              Write down what the expert would say or ask you about your current task list.


              Next, answer the expert's questions or reply to their thoughts.


              Next, revise your task list (remove your now completed tasks), explain your revisions, and write down your current task.
          id: uEWXLi8XmKpaiGzzTFDme
          outgoingConnections:
            - output->"Chat" gHSSvkoWQ76ko-0EvUdZn/prompt
          title: Text
          type: text
          visualData: 7875.686049522404/-255.33900115338668/300/381
        wq5E83vpWqQMlS7AHPnQe:
          data:
            path: $.file
            usePathInput: false
          id: wq5E83vpWqQMlS7AHPnQe
          outgoingConnections:
            - match->"Digest File" cV8OdlEuJDaI2v05bTz5X/file_name
            - match->"Get Nodai File By File Name"
              SNQCA2ZXN7LQRbPd1M6Mx/fileMatch
          title: Extract Object Path
          type: extractObjectPath
          visualData: 5536.689177597142/-434.1124731488284/250/345
        xP7TQ0W3vIsCOyGtZEX5q:
          data:
            path: $.yamlDocument.nextTask
            usePathInput: false
          id: xP7TQ0W3vIsCOyGtZEX5q
          outgoingConnections:
            - match->"Loop Controller" cObRObIxgwrJablQHLGnw/input1
          title: Next Task
          type: extractObjectPath
          visualData: 8854.274134954136/-180.79168735899/250/311
        xe4vk7BUOsT-mnIoEjOha:
          data:
            dataType: string
            id: output
          id: xe4vk7BUOsT-mnIoEjOha
          outgoingConnections: []
          title: Graph Output
          type: graphOutput
          visualData: 2652.651717877761/223.07146808148678/300/161
        z1APvC5m5fW-AIfzoVeox:
          data:
            text: "{{input}}"
          id: z1APvC5m5fW-AIfzoVeox
          outgoingConnections:
            - output->"Chat - Execute Current Task"
              NlqmTc5SUrwdCFOOU26CP/systemPrompt
          title: System
          type: text
          visualData: 3617.485828731137/-47.72596476201433/148.77673982924148/279
        z5IaklOOBA20wwMI0aRkq:
          data:
            path: $.arguments
            usePathInput: false
          id: z5IaklOOBA20wwMI0aRkq
          outgoingConnections:
            - match->"If" CWz3hh4xEp4FLbYV3Tw3v/value
            - match->"If" ItFByM3wvP_dPEoP5D6Mx/value
            - match->"If" Mvbq2URURhELcYZH2EUKw/value
            - match->"If" RNxCaWdA7mOcrJBVUKiqr/value
            - match->"If" Rh5FbKt-FMnow6je0Hm4u/value
            - match->"If" TGIDciqmI5zMTLdVk2T2C/value
            - match->"If" dHDzOydr6Q5IFDBH6z6HU/value
            - match->"Text" 6P04DpTqBQMOYpBfJuurJ/arguments
          title: Extract Object Path
          type: extractObjectPath
          visualData: 4434.673893565094/600.9131958907702/250/385
    zYzI7xjOb0NOj9WPKJOrQ:
      metadata:
        id: zYzI7xjOb0NOj9WPKJOrQ
        name: "**Nodai Helper"
      nodes:
        1K582UsQEjPvjVRvtf4ag:
          data:
            prompt: What is your question for the AI to answer?
            useInput: false
          id: 1K582UsQEjPvjVRvtf4ag
          outgoingConnections:
            - output->"Prompt Question List" 5-z53LCARmASeWft67PEH/request
            - output->"Prompt Question List" KbotaIf5SLrUkOubRve1x/request
            - output->"Prompt Question List" eeqzpCkSxMDTin0sOghhU/request
            - output->"Prompt" YFPEXiTSWNawBUvgAF0_F/question
          title: Question
          type: userInput
          visualData: -2195.2160232826423/-17.22315339809552/378.6378084183709/537
        2AsgAl-VGTQHHKgMYanRs:
          data:
            graphId: d6Pgmz7n8qvXkaNF-2e9P
          id: 2AsgAl-VGTQHHKgMYanRs
          outgoingConnections:
            - files->"Prompt Question List" 5-z53LCARmASeWft67PEH/files
            - files->"Prompt Question List" KbotaIf5SLrUkOubRve1x/files
            - files->"Prompt Question List" eeqzpCkSxMDTin0sOghhU/files
          title: List Nodai Files
          type: subGraph
          visualData: -2127.682357265696/-240.91939301727035/300/536
        4-0zbzUfiK5tL1R22gT2z:
          data:
            text: >-
              Here are some answers given for some questions:


              """

              {{answers}}

              """


              Here is some required changes you need to make to these answers:


              """

              {{feedback}}

              """


              Rewrite these answers with the changes. Additionally, rephrase the answers as statements instead of answers, as if they were found in a technical document for the project.
          id: 4-0zbzUfiK5tL1R22gT2z
          outgoingConnections:
            - output->"Chat" xxSFUqUpaoGhRrU__HAUO/prompt
          title: Text
          type: text
          visualData: 1216.727187150413/-425.279616238741/300/625
        41OQXeM5D_KXd8nvMgD13:
          data:
            objectPath: $.yamlDocument.steps
            rootPropertyName: yamlDocument
          id: 41OQXeM5D_KXd8nvMgD13
          outgoingConnections:
            - output->"Execute Task List" yWAKuzopcX8M17WGEDi-6/task_list
          title: Extract YAML
          type: extractYaml
          visualData: 5077.347198549942/142.63332091512157/179.6401800954518/678
        5-z53LCARmASeWft67PEH:
          data:
            promptText: >-
              {{context}}


              These files are present in the Nodai application:


              {{files}}


              I have the following question or request:


              """

              {{request}}

              """


              You provided me with these questions:


              """

              {{questions}}

              """


              Imagine a scenario where you are the expert on the Nodai application, or a project manager on the project. What are your best answers to these questions? You may say that you do not have enough context to answer any given question. Also, phrase the answers in such a way to include the question in them. Directly answer the questions, as if this is documentation on the project.
            type: user
            useTypeInput: false
          id: 5-z53LCARmASeWft67PEH
          outgoingConnections:
            - output->"Chat" RfEA6xsgL1vEu9oDqRe88/prompt
          title: Prompt Question List
          type: prompt
          visualData: -1488.9069047003868/208.5550898161199/485.03013745499055/546
        6UTrt-tWAv0UmymCmjyGg:
          data: {}
          id: 6UTrt-tWAv0UmymCmjyGg
          outgoingConnections:
            - output->"Coalesce" b3NT35IXmv6oqd2kSitLM/input2
          title: If
          type: if
          visualData: 1111/592/100/596
        7vNigNnYPyllzuKCncBQd:
          data:
            text: >-
              {{context}}


              {{functions}}


              Give me a detailed overview of the steps you will be taking to accomplish this task.


              Make sure you gather context on what you are working with, look at examples, think out loud, give high level tasks, get one or mote examples before creating new files, read files before writing them, and are very detailed.


              You can only use the Chat API to do things like read files and think our loud. You are not working in a repository. There are no branches nor git. You are an AI that is interacting with the system functions only. However, do not include YAML in your response. Your response should be plain English, and may mention the Chat API functions in sentences.


              There are no tests to read. There is no documentation to read. Do not write tests. Do not write documentation.
          id: 7vNigNnYPyllzuKCncBQd
          outgoingConnections:
            - output->"Chat" wnITtjycZ2FJpLYSThTVo/prompt
          title: Text
          type: text
          visualData: 2260.208598489028/282.32332571050824/300/677
        9f2Us4WxeKYfvqMm1_XUZ:
          data:
            prompt: This is an example question?
            useInput: true
          id: 9f2Us4WxeKYfvqMm1_XUZ
          outgoingConnections:
            - questionsAndAnswers->"Coalesce" b3NT35IXmv6oqd2kSitLM/input1
          title: User Input
          type: userInput
          visualData: 1168.2615368939803/321.2573005264068/187/594
        AGmIpXCDl_sHyhLIjtXie:
          data:
            cache: true
            maxTokens: 1024
            model: gpt-3.5-turbo
            temperature: 0
            top_p: 1
            useMaxTokensInput: false
            useModelInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: AGmIpXCDl_sHyhLIjtXie
          outgoingConnections:
            - response->"Prompt Question List" 5-z53LCARmASeWft67PEH/questions
            - response->"Text" VszyH3-ShGM2DBZw1v7VW/questions
          title: Get Question List
          type: chat
          visualData: -885.2318474821926/-230.54556081772057/233.53671241371717/544
        At8pw5kk0skpMBKEBPgZp:
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: At8pw5kk0skpMBKEBPgZp
          outgoingConnections:
            - response->"Extract YAML" 41OQXeM5D_KXd8nvMgD13/input
          title: Chat
          type: chat
          visualData: 4811.227708168585/182.57139818613453/200/658
        B7bKOsVis46f2wZGjHok0:
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: B7bKOsVis46f2wZGjHok0
          outgoingConnections:
            - response->"Text" oFk-Eg5dbpoWQCK9hx_gs/steps
            - response->"Text" sMP1SZoQAgvMH3UtERway/steps
          title: Chat
          type: chat
          visualData: 3464.179659645404/24.612441249477307/200/661
        CQ11jAUax-dtEvf7Jw3Ux:
          data: {}
          id: CQ11jAUax-dtEvf7Jw3Ux
          outgoingConnections:
            - output->"User Input" 9f2Us4WxeKYfvqMm1_XUZ/questions
          title: If
          type: if
          visualData: 1044/328/100/593
        CZqk_fQFmbzNKep6CsIVU:
          data:
            text: You are an advanced AI that makes changes to text.
          id: CZqk_fQFmbzNKep6CsIVU
          outgoingConnections:
            - output->"Chat" xxSFUqUpaoGhRrU__HAUO/systemPrompt
          title: Text
          type: text
          visualData: 1188.9614185009382/-574.3368036976119/300/null
        CfsTEn_pniYVjsyJP-paR:
          data:
            text: "{{context}}"
          id: CfsTEn_pniYVjsyJP-paR
          outgoingConnections:
            - output->"Assemble Prompt" H7i41CNQTM5fM5KcJDwfq/message1
            - output->"Execute Task List" yWAKuzopcX8M17WGEDi-6/context
          title: Text
          type: text
          visualData: 2657.4058456180655/-200.8621114414035/300/643
        DYYQPYCdQyMP2w5Bh2QOO:
          data:
            text: >-
              Here is a list of steps:


              """

              {{steps}}

              """


              Convert this list of steps into a YAML document with the following format:


              ```yaml

              yamlDocument:
                steps:
                  - Step 1
                  - Step 2
              ```


              You must include every single step. Every step MUST be a string, even if the list of steps contains YAML data. 
          id: DYYQPYCdQyMP2w5Bh2QOO
          outgoingConnections:
            - output->"Chat" At8pw5kk0skpMBKEBPgZp/prompt
          title: Text
          type: text
          visualData: 4482.313290057272/41.00054812622696/300/660
        GsiNZWC4KFQa-MiYQcjUG:
          data:
            path: $.yamlDocument.questions
            usePathInput: false
          id: GsiNZWC4KFQa-MiYQcjUG
          outgoingConnections:
            - match->"If" CQ11jAUax-dtEvf7Jw3Ux/value
          title: Extract Object Path
          type: extractObjectPath
          visualData: 627.8152048842937/679.5426701770796/227/583
        H7i41CNQTM5fM5KcJDwfq:
          data: {}
          id: H7i41CNQTM5fM5KcJDwfq
          outgoingConnections:
            - prompt->"Chat" B7bKOsVis46f2wZGjHok0/prompt
          title: Assemble Prompt
          type: assemblePrompt
          visualData: 3184.1516408542043/114.06032561384635/250/642
        Hqpwl6OBs8GdL917iMJNw:
          data:
            path: $.yamlDocument[*].question
            usePathInput: false
          id: Hqpwl6OBs8GdL917iMJNw
          outgoingConnections: []
          title: Extract Object Path
          type: extractObjectPath
          visualData: 725.0975654729407/-223.09365273391288/205.2941012713909/610
        KbotaIf5SLrUkOubRve1x:
          data:
            promptText: >-
              {{context}}


              These files are present in the Nodai application:


              {{files}}


              I have the following question or request:


              """

              {{request}}

              """


              Imagine that you are a developer assigned to this task on the project. You are already aware of how all files in the project already work. But you are likely missing some information to fully complete the request. Please give a list of questions that you would ask the project manager or another developer. Examples are clarifying behavior, or styling, or interaction with the rest of the system, etc.


              You have a maximum of 6 questions.
            type: user
            useTypeInput: false
          id: KbotaIf5SLrUkOubRve1x
          outgoingConnections:
            - output->"Get Question List" AGmIpXCDl_sHyhLIjtXie/message2
            - output->"Get Question List" AGmIpXCDl_sHyhLIjtXie/prompt
            - output->"Prompt Question List" eeqzpCkSxMDTin0sOghhU/questions
          title: Prompt Question List
          type: prompt
          visualData: -1490.4278366754174/-424.16270749159196/485.03013745499055/538
        NCeGFcpBOCqnZICq3vn9m:
          data:
            path: $.yamlDocument['has-questions']
            usePathInput: false
          id: NCeGFcpBOCqnZICq3vn9m
          outgoingConnections:
            - match->"Match" kshUlDyeosoaczIy5rp_C/input
          title: Extract Object Path
          type: extractObjectPath
          visualData: 624.8152048842937/494.54267017707963/172.91706398583165/578
        P3JViEfh4tcb4Kq8X9TSo:
          data:
            text: You are an advanced AI YAML generator tool that takes in inputs and
              produces valid YAML based on a question.
          id: P3JViEfh4tcb4Kq8X9TSo
          outgoingConnections:
            - output->"Chat" noeONFrQRJhnNOIqzfe0v/systemPrompt
            - output->"Chat" wTZ8HUHOjU5COaWxaohVb/systemPrompt
          title: Text
          type: text
          visualData: 30.111429314528237/-82.36315841333024/300/567
        RfEA6xsgL1vEu9oDqRe88:
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-4
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: RfEA6xsgL1vEu9oDqRe88
          outgoingConnections:
            - response->"Prompt Question List" eeqzpCkSxMDTin0sOghhU/answers
            - response->"Text" VszyH3-ShGM2DBZw1v7VW/answers
          title: Chat
          type: chat
          visualData: -860.5478300364443/314.275559094539/200/555
        U0bz-jzJ615Vgm78Lpley:
          data:
            rootPropertyName: yamlDocument
          id: U0bz-jzJ615Vgm78Lpley
          outgoingConnections:
            - output->"Extract Object Path" GsiNZWC4KFQa-MiYQcjUG/object
            - output->"Extract Object Path" NCeGFcpBOCqnZICq3vn9m/object
          title: Extract YAML
          type: extractYaml
          visualData: 466.5038997695302/505.5426701770796/137.3689875131763/577
        V1Gia2nRcaIx-6Zu8YKjx:
          data:
            prompt: This is an example question?
            useInput: true
          id: V1Gia2nRcaIx-6Zu8YKjx
          outgoingConnections:
            - output->"Text" 4-0zbzUfiK5tL1R22gT2z/feedback
          title: User Input
          type: userInput
          visualData: 1683.6344428877155/-62.36167594296539/173.2418484290331/630
        VrlVJ3R4e5XeyDKqHmrjz:
          data:
            text: >-
              Make these changes:


              * Combine steps that are talking about the same file

              * Add steps to look at examples before any steps that write files

              * Remove any steps talking about documentation

              * Remove any steps talking about testing or tests

              * Flatten any nested steps, there should only be one single list, no nested lists.
          id: VrlVJ3R4e5XeyDKqHmrjz
          outgoingConnections:
            - output->"Assemble Prompt" H7i41CNQTM5fM5KcJDwfq/message3
          title: Text
          type: text
          visualData: 2739.943217152992/332.8979395249134/300/641
        VszyH3-ShGM2DBZw1v7VW:
          data:
            text: >-
              Can you please combine these questions and answers into a YAML
              document like this:


              ```yaml

              yamlDocument:
                - question: This is a question
                  answer: This is the answer to the question
                - question: This is a question
                  answer: This is the answer to the question
              ```


              Questions:


              {{questions}}


              Answers:


              {{answers}}



              More Questions and Answers:


              {{qa2}}
          id: VszyH3-ShGM2DBZw1v7VW
          outgoingConnections:
            - output->"Chat" wTZ8HUHOjU5COaWxaohVb/prompt
          title: Text
          type: text
          visualData: 523.5852096259869/-685.6221670073336/300/607
        Y5dFBHDnIc0yTEcNA87W3:
          data:
            text: |
              {{input}}
          id: Y5dFBHDnIc0yTEcNA87W3
          outgoingConnections:
            - output->"Text" pqxbw_BaWOjpkd3G8WnRT/input
          title: Text
          type: text
          visualData: 976.8681178648283/-36.7432284340459/128.14831452392036/680
        YFPEXiTSWNawBUvgAF0_F:
          data:
            promptText: |-
              {{context}}

              I have asked this question:

              """
              {{question}}
              """

              Here are some additional notes:

              {{qanda}}
            type: user
            useTypeInput: false
          id: YFPEXiTSWNawBUvgAF0_F
          outgoingConnections:
            - output->"Text" 7vNigNnYPyllzuKCncBQd/context
            - output->"Text" CfsTEn_pniYVjsyJP-paR/context
          title: Prompt
          type: prompt
          visualData: 1879.8025484562081/-164.03783773403975/null/674
        Z115WdCClukxy33gFZQLm:
          data:
            prompt: This is an example question?
            useInput: true
          id: Z115WdCClukxy33gFZQLm
          outgoingConnections:
            - output->"Text" sMP1SZoQAgvMH3UtERway/feedback
          title: User Input
          type: userInput
          visualData: 3161.412174129719/424.40649291776276/250/668
        ZGfDzvqbUcnQunNNplfBA:
          data:
            text: |-
              Do these answers look sufficient to you? Any changes to make?

              {{answers}}
          id: ZGfDzvqbUcnQunNNplfBA
          outgoingConnections:
            - output->"User Input" V1Gia2nRcaIx-6Zu8YKjx/questions
          title: Text
          type: text
          visualData: 1397.9960111510754/-66.32861624841605/225.89983857684933/631
        _7tr8cgQEbY6mtIakJd-N:
          data:
            text: "You are an advanced automated code developer. You are able to
              autonomously be given a task, and do all relevant development
              tasks assocated with accomplishing that task. "
          id: _7tr8cgQEbY6mtIakJd-N
          outgoingConnections:
            - output->"Chat" RfEA6xsgL1vEu9oDqRe88/systemPrompt
            - output->"Chat" tl7qCr9Y92oWPkXWIZXqr/systemPrompt
            - output->"Get Question List" AGmIpXCDl_sHyhLIjtXie/systemPrompt
          title: Text
          type: text
          visualData: -1600.0835425653868/-718.8751748082445/300/554
        b3NT35IXmv6oqd2kSitLM:
          data: {}
          id: b3NT35IXmv6oqd2kSitLM
          outgoingConnections:
            - output->"Text" VszyH3-ShGM2DBZw1v7VW/qa2
          title: Coalesce
          type: coalesce
          visualData: 1425/423/150/595
        eJ4oKMy5hG51V_63xGOf_:
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: eJ4oKMy5hG51V_63xGOf_
          outgoingConnections:
            - response->"Text" DYYQPYCdQyMP2w5Bh2QOO/steps
          title: Chat
          type: chat
          visualData: 3800.927073268016/366.97261735496113/200/673
        eeqzpCkSxMDTin0sOghhU:
          data:
            promptText: |-
              {{context}}

              I have the following question or request:

              """
              {{request}}
              """

              You asked these questions:

              {{questions}}

              Here are your answers:

              {{answers}}

              Do you have any additional questions? Or followup questions?
            type: user
            useTypeInput: false
          id: eeqzpCkSxMDTin0sOghhU
          outgoingConnections:
            - output->"Chat" tl7qCr9Y92oWPkXWIZXqr/prompt
          title: Prompt Question List
          type: prompt
          visualData: -518.606438840874/61.927116736475924/485.03013745499055/556
        erjoEqmUs-QcW3xxYqdKW:
          data:
            path: $.yamlDocument[*].answer
            usePathInput: false
          id: erjoEqmUs-QcW3xxYqdKW
          outgoingConnections:
            - all_matches->"Text" Y5dFBHDnIc0yTEcNA87W3/input
          title: Extract Object Path
          type: extractObjectPath
          visualData: 722.5034572418909/-34.12901726693207/215.11762658923863/613
        gMe54j9zmTUGFU1t_1viB:
          data:
            text: >-
              You have Chat API functions available to you to use. You may call
              one of these functions to get a response from the system:


              - command: READ_FILES
                arguments:
                  files:
                    - file_name.ts
                description: Reads a file and returns with its contents. Files must be an array with one file per line.
              - command: TAKE_NOTE_FOR_SELF
                arguments:
                  notes: Here is some note to take for youself.
              - command: THINK_OUT_LOUD
                arguments:
                  notes: Here is some text to remember for later.
              - command: WRITE_FILE
                arguments:
                  file: file_name.ts
                contents: |
                  contents of the file
                description: Writes text to the specified file. You must write the entire file contents at once. You must not comment out sections of the file - everything needs to be implemented.
              - command: ASK_FOR_FEEDBACK
                arguments:
                  message: The message for the user
                description: Asks the user questions or for feedback on what you are doing. This can help make sure your plan is good.
          id: gMe54j9zmTUGFU1t_1viB
          outgoingConnections:
            - output->"Prompt" YFPEXiTSWNawBUvgAF0_F/functions
            - output->"Text" 7vNigNnYPyllzuKCncBQd/functions
          title: Text
          type: text
          visualData: 1027.1890763146014/825.3197409716632/529/675
        jj8mjjtKJPlcHzr64IYWp:
          data:
            text: >
              Here is some text:


              """

              {{text}}

              """


              If there are any questions in the document, convert this into a YAML document with the following structure:


              ```yaml

              yamlDocument:
                has-questions: true
                questions: 
                  - This is one questions
                  - This is another question
              ```


              If there are no questions, convert it to this structure:


              ```yaml

              yamlDocument:
                has-questions: false
              ```
          id: jj8mjjtKJPlcHzr64IYWp
          outgoingConnections:
            - output->"Chat" noeONFrQRJhnNOIqzfe0v/prompt
          title: Text
          type: text
          visualData: 321.75173175600827/110.2229471496421/300/565
        klbBsHw6M8_i7Jz1aYovd:
          data:
            rootPropertyName: yamlDocument
          id: klbBsHw6M8_i7Jz1aYovd
          outgoingConnections:
            - output->"Extract Object Path" Hqpwl6OBs8GdL917iMJNw/object
            - output->"Extract Object Path" erjoEqmUs-QcW3xxYqdKW/object
          title: Extract YAML
          type: extractYaml
          visualData: 529.2154812835439/-228.29406204867558/147.72528347633283/609
        koUO8WjYqJBw5m1DqUzgv:
          data:
            text: You are an advanced AI for creating and updating task lists to accomplish
              programming tasks. You interact with the chat API to accomplish
              your tasks. You use the chat API to take notes for yourself and to
              think out loud about what you are doing. You are very detailed in
              your explanations and steps.
          id: koUO8WjYqJBw5m1DqUzgv
          outgoingConnections:
            - output->"Chat" B7bKOsVis46f2wZGjHok0/systemPrompt
            - output->"Chat" wnITtjycZ2FJpLYSThTVo/systemPrompt
          title: Text
          type: text
          visualData: 2274.1767851688805/-464.34829550512757/300/618
        kshUlDyeosoaczIy5rp_C:
          data:
            caseCount: 2
            cases:
              - "true"
              - "false"
          id: kshUlDyeosoaczIy5rp_C
          outgoingConnections:
            - case1->"If" CQ11jAUax-dtEvf7Jw3Ux/if
            - case2->"If" 6UTrt-tWAv0UmymCmjyGg/if
          title: Match
          type: match
          visualData: 815.6277136675187/481.2361797361615/177.18989855369773/581
        mpzTKKufP2XKwIToZ6C4x:
          data:
            promptText: "{{input}}"
            type: assistant
            useTypeInput: false
          id: mpzTKKufP2XKwIToZ6C4x
          outgoingConnections:
            - output->"Assemble Prompt" H7i41CNQTM5fM5KcJDwfq/message2
          title: Prompt
          type: prompt
          visualData: 2936.650166754064/58.192032198931265/132/639
        nmuXZqWZghE8cxrYC1BRW:
          data:
            text: None
          id: nmuXZqWZghE8cxrYC1BRW
          outgoingConnections:
            - output->"If" 6UTrt-tWAv0UmymCmjyGg/value
          title: Text
          type: text
          visualData: 932/700/138/590
        noeONFrQRJhnNOIqzfe0v:
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: noeONFrQRJhnNOIqzfe0v
          outgoingConnections:
            - response->"Extract YAML" U0bz-jzJ615Vgm78Lpley/input
          title: Chat
          type: chat
          visualData: 693.2893628468472/281.2261400604991/200/566
        oFk-Eg5dbpoWQCK9hx_gs:
          data:
            text: |-
              Here is the list of steps I will be doing:

              {{steps}}

              Do you have any changes I should make?
          id: oFk-Eg5dbpoWQCK9hx_gs
          outgoingConnections:
            - output->"User Input" Z115WdCClukxy33gFZQLm/questions
          title: Text
          type: text
          visualData: 3704.1400052653166/5.251166476408985/237.2099079807631/663
        pqxbw_BaWOjpkd3G8WnRT:
          data:
            text: "{{input}}"
          id: pqxbw_BaWOjpkd3G8WnRT
          outgoingConnections:
            - output->"Text" 4-0zbzUfiK5tL1R22gT2z/answers
            - output->"Text" ZGfDzvqbUcnQunNNplfBA/answers
          title: Text
          type: text
          visualData: 1141.592660493523/-32.3165797585659/109.04166402015858/679
        sMP1SZoQAgvMH3UtERway:
          data:
            text: |-
              Here is a list of steps:

              """
              {{steps}}
              """

              Here are required changes to make to the steps:

              """
              {{feedback}}
              """

              Please write the steps, with the required changes made. 
          id: sMP1SZoQAgvMH3UtERway
          outgoingConnections:
            - output->"Chat" eJ4oKMy5hG51V_63xGOf_/prompt
          title: Text
          type: text
          visualData: 3449.6025964744235/222.3511968045764/300/669
        tl7qCr9Y92oWPkXWIZXqr:
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: tl7qCr9Y92oWPkXWIZXqr
          outgoingConnections:
            - response->"Text" jj8mjjtKJPlcHzr64IYWp/text
          title: Chat
          type: chat
          visualData: 58.74579830891864/277.5903593432446/200/558
        wTZ8HUHOjU5COaWxaohVb:
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: wTZ8HUHOjU5COaWxaohVb
          outgoingConnections:
            - response->"Extract YAML" klbBsHw6M8_i7Jz1aYovd/input
          title: Chat
          type: chat
          visualData: 852.9285172548934/-523.9745290372787/200/608
        wVMd8FhPjQOvVlnHxZR_L:
          data:
            text: You are an advanced AI that combines two pieces of text. You do not leave
              any details out.
          id: wVMd8FhPjQOvVlnHxZR_L
          outgoingConnections:
            - output->"Chat" eJ4oKMy5hG51V_63xGOf_/systemPrompt
          title: Text
          type: text
          visualData: 3089.9485001028697/288.8805450446473/304.0744894139352/672
        wnITtjycZ2FJpLYSThTVo:
          data:
            cache: true
            frequencyPenalty: 0.2
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0.2
            stop: ""
            temperature: 0.2
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: wnITtjycZ2FJpLYSThTVo
          outgoingConnections:
            - response->"Prompt" mpzTKKufP2XKwIToZ6C4x/input
            - response->"Text" VrlVJ3R4e5XeyDKqHmrjz/steps
          title: Chat
          type: chat
          visualData: 2665.869382184409/17.864793208914165/200/635
        xxSFUqUpaoGhRrU__HAUO:
          data:
            cache: true
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-4
            presencePenalty: 0
            stop: ""
            temperature: 0
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: xxSFUqUpaoGhRrU__HAUO
          outgoingConnections:
            - response->"Prompt" YFPEXiTSWNawBUvgAF0_F/qanda
          title: Chat
          type: chat
          visualData: 1571.1932071403805/-284.56226186771005/200/628
        yWAKuzopcX8M17WGEDi-6:
          data:
            graphId: yoe9VPnjUULQacHSolgiL
          id: yWAKuzopcX8M17WGEDi-6
          outgoingConnections: []
          title: Execute Task List
          type: subGraph
          visualData: 5580.14785799502/-48.341959799326155/300/655
        zE8Wp38T4EAR0doXvx4zT:
          data:
            text: You are an advanced AI YAML generator tool that takes in inputs and
              produces valid YAML based on a question. You leave nothing out.
              Your conversion is verbatim.
          id: zE8Wp38T4EAR0doXvx4zT
          outgoingConnections:
            - output->"Chat" At8pw5kk0skpMBKEBPgZp/systemPrompt
          title: Text
          type: text
          visualData: 4488.321151175641/-135.07604537061488/300/659
        zc7_qtAZEqumCFsUYVqt7:
          data:
            promptText: >-
              I am working on developing an AI storyboarding tool that allows
              users to create a series of prompts for a language model in a
              choose-your-own-adventure format. The tool is inspired by
              node-based editors, like the one found in Blender, where users can
              create nodes on a page that have inputs and outputs that can be
              connected by wires to form a web of connections between the
              prompts and the AI. Each node can be edited, and when editing, a
              larger window pops up with a text editor where users can tweak
              various aspects of the prompt that will be fed to the AI. This
              tool will provide a user-friendly interface for crafting
              interactive stories with an AI language model. Here is a tree of
              my current files for context. If you would like the contents of
              any of these files, please ask. The app is dark-themed and the
              colors are available in index.css. I'm using Emotion for CSS.


              Every node is split into two files. The file in `core`, such as `core/src/model/nodes/TextNode.ts`, contains the main implementation of the node's functionality. The file in `app`, such as `app/src/components/nodes/TextNode.tsx`, contains three react UI components related to showing the node in the application. 
            type: user
            useTypeInput: false
          id: zc7_qtAZEqumCFsUYVqt7
          outgoingConnections:
            - output->"Get Question List" AGmIpXCDl_sHyhLIjtXie/message1
            - output->"Prompt Question List" 5-z53LCARmASeWft67PEH/context
            - output->"Prompt Question List" KbotaIf5SLrUkOubRve1x/context
            - output->"Prompt Question List" eeqzpCkSxMDTin0sOghhU/context
            - output->"Prompt" YFPEXiTSWNawBUvgAF0_F/context
          title: Base Context
          type: prompt
          visualData: -2228.0616865013358/-551.6344392043734/528/535
    zbg_gBTAuWckH0VQ_hTMb:
      metadata:
        description: ""
        id: zbg_gBTAuWckH0VQ_hTMb
        name: Untitled Graph
      nodes:
        0G74tPilGW8YNgcJiXQLz:
          data:
            text: You are a nonsense text generator. The user is testing message lengths, so
              needs a bunch of stuff that doesn't make sense.
          id: 0G74tPilGW8YNgcJiXQLz
          outgoingConnections:
            - output->"Chat" GNAIB_ctKlUx7UNiOxxOX/systemPrompt
          title: Text
          type: text
          visualData: 759.3152628212886/473.7600118789283/300/20
        ApW33SmfgIQcxEZz9aYQo:
          data: {}
          id: ApW33SmfgIQcxEZz9aYQo
          outgoingConnections:
            - prompt->"Loop Controller" d9_44wwp_vNnfBXa6KOqL/input1
            - prompt->"Trim Chat Messages" ur8kbx3l8kuNYovsK5OYL/input
          title: Assemble Prompt
          type: assemblePrompt
          visualData: 391/760/250/11
        GNAIB_ctKlUx7UNiOxxOX:
          data:
            cache: false
            frequencyPenalty: 0
            maxTokens: 1024
            model: gpt-3.5-turbo
            presencePenalty: 0
            stop: ""
            temperature: 0.5
            top_p: 1
            useFrequencyPenaltyInput: false
            useMaxTokensInput: false
            useModelInput: false
            usePresencePenaltyInput: false
            useStop: false
            useStopInput: false
            useTemperatureInput: false
            useTopP: false
            useTopPInput: false
            useUseTopPInput: false
          id: GNAIB_ctKlUx7UNiOxxOX
          outgoingConnections:
            - response->"Prompt" cufCkdmxm8bHkgxn8h2TM/input
          title: Chat
          type: chat
          visualData: 1327.4502985058366/682.2456434664496/200/17
        cufCkdmxm8bHkgxn8h2TM:
          data:
            promptText: "{{input}}"
            type: assistant
            useTypeInput: false
          id: cufCkdmxm8bHkgxn8h2TM
          outgoingConnections:
            - output->"Loop Controller" d9_44wwp_vNnfBXa6KOqL/input2
          title: Prompt
          type: prompt
          visualData: 1614.8081371325602/696.448398816932/null/21
        d9_44wwp_vNnfBXa6KOqL:
          data: {}
          id: d9_44wwp_vNnfBXa6KOqL
          outgoingConnections:
            - break->"Graph Output" kkvZiHHgqJDb07PY_txkb/value
            - output1->"Assemble Prompt" ApW33SmfgIQcxEZz9aYQo/message1
            - output2->"Assemble Prompt" ApW33SmfgIQcxEZz9aYQo/message2
          title: Loop Controller
          type: loopController
          visualData: -1/514/250/6
        kkvZiHHgqJDb07PY_txkb:
          data:
            dataType: string
            id: output
          id: kkvZiHHgqJDb07PY_txkb
          outgoingConnections: []
          title: Graph Output
          type: graphOutput
          visualData: 432/363/300/15
        ur8kbx3l8kuNYovsK5OYL:
          data:
            maxTokenCount: 1024
            model: gpt-3.5-turbo
            removeFromBeginning: true
          id: ur8kbx3l8kuNYovsK5OYL
          outgoingConnections:
            - trimmed->"Chat" GNAIB_ctKlUx7UNiOxxOX/prompt
          title: Trim Chat Messages
          type: trimChatMessages
          visualData: 786/690/200/16
    zdzAYsxAtpaIcWCfgFZpz:
      metadata:
        description: ""
        id: zdzAYsxAtpaIcWCfgFZpz
        name: Test Digest
      nodes:
        LCcfd6Kcj2Rw0q1glhv6O:
          data:
            graphId: HXjZhpWO0hluMiDY6pneE
          id: LCcfd6Kcj2Rw0q1glhv6O
          outgoingConnections: []
          title: Digest File
          type: subGraph
          visualData: 589/377/300/3
        tNLFIIZaLhPiLXTzaSX4V:
          data:
            prompt: File
            useInput: false
          id: tNLFIIZaLhPiLXTzaSX4V
          outgoingConnections:
            - output->"Digest File" LCcfd6Kcj2Rw0q1glhv6O/fileMatch
          title: User Input
          type: userInput
          visualData: 284/377/250/2
  metadata:
    description: Project for Nodai itself
    id: KufOYoZj8bXSme0gx4W-e
    title: Nodai
